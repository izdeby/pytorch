#include <ATen/LegacyTHFunctionsCUDA.h>

// @generated by aten/src/ATen/gen.py

#include <ATen/ATen.h>
#include <ATen/Utils.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/CUDAGenerator.h>
#include <ATen/ExpandUtils.h>
#include <ATen/core/EnableNamedTensor.h>
#include <THC/THC.h>
#include <THC/THCTensor.hpp>
#include <THCUNN/THCUNN.h>
#undef THNN_
#undef THCIndexTensor_
#include <ATen/DeviceGuard.h>
#include <ATen/cuda/ATenCUDAGeneral.h>
#include <ATen/cuda/CUDADevice.h>
#include <ATen/cuda/CUDAContext.h>

namespace at {
namespace native {
namespace legacy {
namespace cuda {

namespace {
  ScalarType infer_scalar_type(const Tensor & t) {
    return t.scalar_type();
  }
  ScalarType infer_scalar_type(const TensorList & tl) {
    TORCH_CHECK(tl.size() > 0, "expected a non-empty list of Tensors");
    return tl[0].scalar_type();
  }

  TensorOptions options(ScalarType s) {
    return TensorOptions().dtype(s)
                          .device(DeviceType::CUDA)
                          .layout(kStrided)
                          .is_variable(false);
  }

  Allocator* allocator() {
    return at::cuda::getCUDADeviceAllocator();
  }
}

Tensor & _th_set_(Tensor & self, Storage source) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Bool));
            THCudaBoolTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Byte));
            THCudaByteTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Char);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Char));
            THCudaCharTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Double);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Double));
            THCudaDoubleTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Float);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Float));
            THCudaTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Int);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Int));
            THCudaIntTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Long));
            THCudaLongTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Short);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Short));
            THCudaShortTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Half);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Half));
            THCudaHalfTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_set_(Tensor & self, Storage source, int64_t storage_offset, IntArrayRef size, IntArrayRef stride) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Bool));
            THCudaBoolTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Byte));
            THCudaByteTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Char);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Char));
            THCudaCharTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Double);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Double));
            THCudaDoubleTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Float);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Float));
            THCudaTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Int);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Int));
            THCudaIntTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Long));
            THCudaLongTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Short);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Short));
            THCudaShortTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Half);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CUDA, at::scalarTypeToTypeMeta(ScalarType::Half));
            THCudaHalfTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_set_(Tensor & self, const Tensor & source) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source, "source", 2, "_th_set_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source, "source", 2, "_th_set_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Char);
            auto source_ = checked_tensor_unwrap(source, "source", 2, "_th_set_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Double);
            auto source_ = checked_tensor_unwrap(source, "source", 2, "_th_set_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Float);
            auto source_ = checked_tensor_unwrap(source, "source", 2, "_th_set_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Int);
            auto source_ = checked_tensor_unwrap(source, "source", 2, "_th_set_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 2, "_th_set_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Short);
            auto source_ = checked_tensor_unwrap(source, "source", 2, "_th_set_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Half);
            auto source_ = checked_tensor_unwrap(source, "source", 2, "_th_set_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_set(globalContext().getTHCState(), self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_set_(Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_set_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_fill_(Tensor & self, Scalar value) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fill_", false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toBool();
            THCudaBoolTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fill_", false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toByte();
            THCudaByteTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fill_", false, Backend::CUDA, ScalarType::Char);
            auto value_ = value.toChar();
            THCudaCharTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fill_", false, Backend::CUDA, ScalarType::Double);
            auto value_ = value.toDouble();
            THCudaDoubleTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fill_", false, Backend::CUDA, ScalarType::Float);
            auto value_ = value.toFloat();
            THCudaTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fill_", false, Backend::CUDA, ScalarType::Int);
            auto value_ = value.toInt();
            THCudaIntTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fill_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toLong();
            THCudaLongTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fill_", false, Backend::CUDA, ScalarType::Short);
            auto value_ = value.toShort();
            THCudaShortTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fill_", false, Backend::CUDA, ScalarType::Half);
            auto value_ = value.toHalf();
            THCudaHalfTensor_fill(globalContext().getTHCState(), self_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_fill_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_fill_(Tensor & self, const Tensor & value) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return _th_fill_(self, value.item());
    }
    AT_ERROR("_th_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
bool _th_is_set_to(const Tensor & self, const Tensor & tensor) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_is_set_to", false, Backend::CUDA, ScalarType::Bool);
            auto tensor_ = checked_tensor_unwrap(tensor, "tensor", 2, "_th_is_set_to", false, Backend::CUDA, ScalarType::Bool);
            return THCudaBoolTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_is_set_to", false, Backend::CUDA, ScalarType::Byte);
            auto tensor_ = checked_tensor_unwrap(tensor, "tensor", 2, "_th_is_set_to", false, Backend::CUDA, ScalarType::Byte);
            return THCudaByteTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_is_set_to", false, Backend::CUDA, ScalarType::Char);
            auto tensor_ = checked_tensor_unwrap(tensor, "tensor", 2, "_th_is_set_to", false, Backend::CUDA, ScalarType::Char);
            return THCudaCharTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_is_set_to", false, Backend::CUDA, ScalarType::Double);
            auto tensor_ = checked_tensor_unwrap(tensor, "tensor", 2, "_th_is_set_to", false, Backend::CUDA, ScalarType::Double);
            return THCudaDoubleTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_is_set_to", false, Backend::CUDA, ScalarType::Float);
            auto tensor_ = checked_tensor_unwrap(tensor, "tensor", 2, "_th_is_set_to", false, Backend::CUDA, ScalarType::Float);
            return THCudaTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_is_set_to", false, Backend::CUDA, ScalarType::Int);
            auto tensor_ = checked_tensor_unwrap(tensor, "tensor", 2, "_th_is_set_to", false, Backend::CUDA, ScalarType::Int);
            return THCudaIntTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_is_set_to", false, Backend::CUDA, ScalarType::Long);
            auto tensor_ = checked_tensor_unwrap(tensor, "tensor", 2, "_th_is_set_to", false, Backend::CUDA, ScalarType::Long);
            return THCudaLongTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_is_set_to", false, Backend::CUDA, ScalarType::Short);
            auto tensor_ = checked_tensor_unwrap(tensor, "tensor", 2, "_th_is_set_to", false, Backend::CUDA, ScalarType::Short);
            return THCudaShortTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_is_set_to", false, Backend::CUDA, ScalarType::Half);
            auto tensor_ = checked_tensor_unwrap(tensor, "tensor", 2, "_th_is_set_to", false, Backend::CUDA, ScalarType::Half);
            return THCudaHalfTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
            break;
        }
        default:
            AT_ERROR("_th_is_set_to not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_masked_fill_(Tensor & self, const Tensor & mask, Scalar value) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_mask;
    std::tie(b_mask) = expand_inplace(self, mask, "_th_masked_fill_");
    return s__th_masked_fill_(self, b_mask, value);
}
Tensor & s__th_masked_fill_(Tensor & self, const Tensor & mask, Scalar value) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Bool);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toBool();
            THCudaBoolTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toByte();
            THCudaByteTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toChar();
            THCudaCharTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toDouble();
            THCudaDoubleTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toFloat();
            THCudaTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toInt();
            THCudaIntTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toLong();
            THCudaLongTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toShort();
            THCudaShortTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, Backend::CUDA, ScalarType::Byte);
            auto value_ = value.toHalf();
            THCudaHalfTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_fill_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_masked_fill_(Tensor & self, const Tensor & mask, const Tensor & value) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_mask;
    std::tie(b_mask) = expand_inplace(self, mask, "_th_masked_fill_");
    return s__th_masked_fill_(self, b_mask, value);
}
Tensor & s__th_masked_fill_(Tensor & self, const Tensor & mask, const Tensor & value) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return _th_masked_fill_(self, mask, value.item());
    }
    AT_ERROR("_th_masked_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
Tensor & _th_masked_fill_bool_(Tensor & self, const Tensor & mask, Scalar value) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_mask;
    std::tie(b_mask) = expand_inplace(self, mask, "_th_masked_fill_bool_");
    return s__th_masked_fill_bool_(self, b_mask, value);
}
Tensor & s__th_masked_fill_bool_(Tensor & self, const Tensor & mask, Scalar value) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toBool();
            THCudaBoolTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toByte();
            THCudaByteTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toChar();
            THCudaCharTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toDouble();
            THCudaDoubleTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toFloat();
            THCudaTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toInt();
            THCudaIntTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toLong();
            THCudaLongTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toShort();
            THCudaShortTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto value_ = value.toHalf();
            THCudaHalfTensor_maskedFillBool(globalContext().getTHCState(), self_, mask_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_fill_bool_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_masked_fill_bool_(Tensor & self, const Tensor & mask, const Tensor & value) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_mask;
    std::tie(b_mask) = expand_inplace(self, mask, "_th_masked_fill_bool_");
    return s__th_masked_fill_bool_(self, b_mask, value);
}
Tensor & s__th_masked_fill_bool_(Tensor & self, const Tensor & mask, const Tensor & value) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return _th_masked_fill_bool_(self, mask, value.item());
    }
    AT_ERROR("_th_masked_fill_bool_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
Tensor & _th_masked_scatter_(Tensor & self, const Tensor & mask, const Tensor & source) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_mask;
    std::tie(b_mask) = expand_inplace(self, mask, "_th_masked_scatter_");
    return s__th_masked_scatter_(self, b_mask, source);
}
Tensor & s__th_masked_scatter_(Tensor & self, const Tensor & mask, const Tensor & source) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Bool);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_scatter_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_masked_scatter_bool_(Tensor & self, const Tensor & mask, const Tensor & source) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_mask;
    std::tie(b_mask) = expand_inplace(self, mask, "_th_masked_scatter_bool_");
    return s__th_masked_scatter_bool_(self, b_mask, source);
}
Tensor & s__th_masked_scatter_bool_(Tensor & self, const Tensor & mask, const Tensor & source) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_maskedCopyBool(globalContext().getTHCState(), self_, mask_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_scatter_bool_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_masked_select_out(Tensor & result, const Tensor & self, const Tensor & mask) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_mask;
    std::tie(b_self, b_mask) = expand_outplace(self, mask, "_th_masked_select_out");
    return s__th_masked_select_out(result, b_self, b_mask);
}
Tensor & s__th_masked_select_out(Tensor & result, const Tensor & self, const Tensor & mask) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Bool);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaBoolTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaCharTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaDoubleTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaIntTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaLongTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaShortTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaHalfTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_masked_select(const Tensor & self, const Tensor & mask) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_mask;
    std::tie(b_self, b_mask) = expand_outplace(self, mask, "_th_masked_select");
    return s__th_masked_select(b_self, b_mask);
}
Tensor s__th_masked_select(const Tensor & self, const Tensor & mask) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select", false, Backend::CUDA, ScalarType::Bool);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, Backend::CUDA, ScalarType::Byte);
            THCudaBoolTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select", false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select", false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, Backend::CUDA, ScalarType::Byte);
            THCudaCharTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select", false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, Backend::CUDA, ScalarType::Byte);
            THCudaDoubleTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select", false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, Backend::CUDA, ScalarType::Byte);
            THCudaTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select", false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, Backend::CUDA, ScalarType::Byte);
            THCudaIntTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select", false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, Backend::CUDA, ScalarType::Byte);
            THCudaLongTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select", false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, Backend::CUDA, ScalarType::Byte);
            THCudaShortTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select", false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, Backend::CUDA, ScalarType::Byte);
            THCudaHalfTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_masked_select_bool_out(Tensor & result, const Tensor & self, const Tensor & mask) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_mask;
    std::tie(b_self, b_mask) = expand_outplace(self, mask, "_th_masked_select_bool_out");
    return s__th_masked_select_bool_out(result, b_self, b_mask);
}
Tensor & s__th_masked_select_bool_out(Tensor & result, const Tensor & self, const Tensor & mask) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Bool);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaByteTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaCharTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaDoubleTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaIntTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaLongTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaShortTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaHalfTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select_bool_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_masked_select_bool(const Tensor & self, const Tensor & mask) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_mask;
    std::tie(b_self, b_mask) = expand_outplace(self, mask, "_th_masked_select_bool");
    return s__th_masked_select_bool(b_self, b_mask);
}
Tensor s__th_masked_select_bool(const Tensor & self, const Tensor & mask) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Bool);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Byte);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Bool);
            THCudaByteTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Char);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Bool);
            THCudaCharTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Double);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Bool);
            THCudaDoubleTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Bool);
            THCudaTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Int);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Bool);
            THCudaIntTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Bool);
            THCudaLongTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Short);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Bool);
            THCudaShortTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Half);
            auto mask_ = checked_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, Backend::CUDA, ScalarType::Bool);
            THCudaHalfTensor_maskedSelectBool(globalContext().getTHCState(), result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select_bool not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_nonzero_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_nonzero_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_nonzero(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_nonzero", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_nonzero(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_nonzero not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_clone(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clone", false, Backend::CUDA, ScalarType::Bool);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaBoolTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clone", false, Backend::CUDA, ScalarType::Byte);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaByteTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clone", false, Backend::CUDA, ScalarType::Char);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaCharTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clone", false, Backend::CUDA, ScalarType::Double);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaDoubleTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clone", false, Backend::CUDA, ScalarType::Float);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clone", false, Backend::CUDA, ScalarType::Int);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaIntTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clone", false, Backend::CUDA, ScalarType::Long);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaLongTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clone", false, Backend::CUDA, ScalarType::Short);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaShortTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clone", false, Backend::CUDA, ScalarType::Half);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaHalfTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        default:
            AT_ERROR("_th_clone not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_resize_as_(Tensor & self, const Tensor & the_template) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_resize_as_", false, Backend::CUDA, ScalarType::Bool);
            auto the_template_ = checked_tensor_unwrap(the_template, "the_template", 2, "_th_resize_as_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_resize_as_", false, Backend::CUDA, ScalarType::Byte);
            auto the_template_ = checked_tensor_unwrap(the_template, "the_template", 2, "_th_resize_as_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_resize_as_", false, Backend::CUDA, ScalarType::Char);
            auto the_template_ = checked_tensor_unwrap(the_template, "the_template", 2, "_th_resize_as_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_resize_as_", false, Backend::CUDA, ScalarType::Double);
            auto the_template_ = checked_tensor_unwrap(the_template, "the_template", 2, "_th_resize_as_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_resize_as_", false, Backend::CUDA, ScalarType::Float);
            auto the_template_ = checked_tensor_unwrap(the_template, "the_template", 2, "_th_resize_as_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_resize_as_", false, Backend::CUDA, ScalarType::Int);
            auto the_template_ = checked_tensor_unwrap(the_template, "the_template", 2, "_th_resize_as_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_resize_as_", false, Backend::CUDA, ScalarType::Long);
            auto the_template_ = checked_tensor_unwrap(the_template, "the_template", 2, "_th_resize_as_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_resize_as_", false, Backend::CUDA, ScalarType::Short);
            auto the_template_ = checked_tensor_unwrap(the_template, "the_template", 2, "_th_resize_as_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_resize_as_", false, Backend::CUDA, ScalarType::Half);
            auto the_template_ = checked_tensor_unwrap(the_template, "the_template", 2, "_th_resize_as_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_resize_as_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_index_select_out(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_index_select_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select_out", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select_out", false, Backend::CUDA, ScalarType::Long);
            THCudaBoolTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_index_select_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select_out", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select_out", false, Backend::CUDA, ScalarType::Long);
            THCudaByteTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_index_select_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select_out", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select_out", false, Backend::CUDA, ScalarType::Long);
            THCudaCharTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_index_select_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select_out", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select_out", false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_index_select_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select_out", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select_out", false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_index_select_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select_out", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select_out", false, Backend::CUDA, ScalarType::Long);
            THCudaIntTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_index_select_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select_out", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_index_select_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select_out", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select_out", false, Backend::CUDA, ScalarType::Long);
            THCudaShortTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_index_select_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select_out", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select_out", false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_index_select_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_index_select(const Tensor & self, int64_t dim, const Tensor & index) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select", false, Backend::CUDA, ScalarType::Long);
            THCudaBoolTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select", false, Backend::CUDA, ScalarType::Long);
            THCudaByteTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select", false, Backend::CUDA, ScalarType::Long);
            THCudaCharTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select", false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select", false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select", false, Backend::CUDA, ScalarType::Long);
            THCudaIntTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select", false, Backend::CUDA, ScalarType::Long);
            THCudaShortTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_select", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_select", false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_index_select not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_index_copy_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_index_copy_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_take_out(Tensor & result, const Tensor & self, const Tensor & index) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_take_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take_out", false, Backend::CUDA, ScalarType::Bool);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take_out", false, Backend::CUDA, ScalarType::Long);
            THCudaBoolTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_take_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take_out", false, Backend::CUDA, ScalarType::Byte);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take_out", false, Backend::CUDA, ScalarType::Long);
            THCudaByteTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_take_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take_out", false, Backend::CUDA, ScalarType::Char);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take_out", false, Backend::CUDA, ScalarType::Long);
            THCudaCharTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_take_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take_out", false, Backend::CUDA, ScalarType::Double);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take_out", false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_take_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take_out", false, Backend::CUDA, ScalarType::Float);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take_out", false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_take_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take_out", false, Backend::CUDA, ScalarType::Int);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take_out", false, Backend::CUDA, ScalarType::Long);
            THCudaIntTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_take_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take_out", false, Backend::CUDA, ScalarType::Long);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_take_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take_out", false, Backend::CUDA, ScalarType::Short);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take_out", false, Backend::CUDA, ScalarType::Long);
            THCudaShortTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_take_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take_out", false, Backend::CUDA, ScalarType::Half);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take_out", false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_take_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_take(const Tensor & self, const Tensor & index) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take", false, Backend::CUDA, ScalarType::Bool);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take", false, Backend::CUDA, ScalarType::Long);
            THCudaBoolTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take", false, Backend::CUDA, ScalarType::Byte);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take", false, Backend::CUDA, ScalarType::Long);
            THCudaByteTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take", false, Backend::CUDA, ScalarType::Char);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take", false, Backend::CUDA, ScalarType::Long);
            THCudaCharTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take", false, Backend::CUDA, ScalarType::Double);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take", false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take", false, Backend::CUDA, ScalarType::Float);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take", false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take", false, Backend::CUDA, ScalarType::Int);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take", false, Backend::CUDA, ScalarType::Long);
            THCudaIntTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take", false, Backend::CUDA, ScalarType::Long);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take", false, Backend::CUDA, ScalarType::Short);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take", false, Backend::CUDA, ScalarType::Long);
            THCudaShortTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_take", false, Backend::CUDA, ScalarType::Half);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_take", false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_take(globalContext().getTHCState(), result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_take not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_put_(Tensor & self, const Tensor & index, const Tensor & source, bool accumulate) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_put_", false, Backend::CUDA, ScalarType::Bool);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_put_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_put_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_put_", false, Backend::CUDA, ScalarType::Byte);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_put_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_put_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_put_", false, Backend::CUDA, ScalarType::Char);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_put_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_put_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_put_", false, Backend::CUDA, ScalarType::Double);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_put_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_put_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_put_", false, Backend::CUDA, ScalarType::Float);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_put_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_put_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_put_", false, Backend::CUDA, ScalarType::Int);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_put_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_put_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_put_", false, Backend::CUDA, ScalarType::Long);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_put_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_put_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_put_", false, Backend::CUDA, ScalarType::Short);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_put_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_put_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_put_", false, Backend::CUDA, ScalarType::Half);
            auto index_ = checked_tensor_unwrap(index, "index", 2, "_th_put_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 3, "_th_put_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_put_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_index_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_add_", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_add_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_add_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_add_", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_add_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_add_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_add_", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_add_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_add_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_add_", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_add_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_add_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_add_", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_add_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_add_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_add_", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_add_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_add_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_add_", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_add_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_add_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_add_", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_add_", false, Backend::CUDA, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source, "source", 4, "_th_index_add_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_index_add_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_index_fill_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toBool();
            THCudaBoolTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toByte();
            THCudaByteTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toChar();
            THCudaCharTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toDouble();
            THCudaDoubleTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toFloat();
            THCudaTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toInt();
            THCudaIntTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toLong();
            THCudaLongTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toShort();
            THCudaShortTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toHalf();
            THCudaHalfTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_index_fill_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_index_fill_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & value) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return _th_index_fill_(self, dim, index, value.item());
    }
    AT_ERROR("_th_index_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
Tensor & _th_unfold_out(Tensor & result, const Tensor & self, int64_t dimension, int64_t size, int64_t step) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_unfold_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold_out", false, Backend::CUDA, ScalarType::Bool);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaBoolTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_unfold_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold_out", false, Backend::CUDA, ScalarType::Byte);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaByteTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_unfold_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold_out", false, Backend::CUDA, ScalarType::Char);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaCharTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_unfold_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold_out", false, Backend::CUDA, ScalarType::Double);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaDoubleTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_unfold_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold_out", false, Backend::CUDA, ScalarType::Float);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_unfold_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold_out", false, Backend::CUDA, ScalarType::Int);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaIntTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_unfold_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold_out", false, Backend::CUDA, ScalarType::Long);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaLongTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_unfold_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold_out", false, Backend::CUDA, ScalarType::Short);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaShortTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_unfold_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold_out", false, Backend::CUDA, ScalarType::Half);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaHalfTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_unfold_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_unfold(const Tensor & self, int64_t dimension, int64_t size, int64_t step) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold", false, Backend::CUDA, ScalarType::Bool);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaBoolTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold", false, Backend::CUDA, ScalarType::Byte);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaByteTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold", false, Backend::CUDA, ScalarType::Char);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaCharTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold", false, Backend::CUDA, ScalarType::Double);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaDoubleTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold", false, Backend::CUDA, ScalarType::Float);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold", false, Backend::CUDA, ScalarType::Int);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaIntTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold", false, Backend::CUDA, ScalarType::Long);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaLongTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold", false, Backend::CUDA, ScalarType::Short);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaShortTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_unfold", false, Backend::CUDA, ScalarType::Half);
            dimension = maybe_wrap_dim(dimension, self_);
            THCudaHalfTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_unfold not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_scatter_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    if (src.dim() == 0) {
        return _th_scatter_(self, dim, index, src.item());
    }
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_scatter_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_scatter_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toBool();
            THCudaBoolTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toByte();
            THCudaByteTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toChar();
            THCudaCharTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toDouble();
            THCudaDoubleTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toFloat();
            THCudaTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toInt();
            THCudaIntTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toLong();
            THCudaLongTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toShort();
            THCudaShortTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_", false, Backend::CUDA, ScalarType::Long);
            auto value_ = value.toHalf();
            THCudaHalfTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_scatter_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_scatter_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_scatter_add_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_gather_out(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gather_out", false, Backend::CUDA, ScalarType::Bool);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather_out", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather_out", false, Backend::CUDA, ScalarType::Long);
            THCudaBoolTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gather_out", false, Backend::CUDA, ScalarType::Byte);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather_out", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather_out", false, Backend::CUDA, ScalarType::Long);
            THCudaByteTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gather_out", false, Backend::CUDA, ScalarType::Char);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather_out", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather_out", false, Backend::CUDA, ScalarType::Long);
            THCudaCharTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gather_out", false, Backend::CUDA, ScalarType::Double);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather_out", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather_out", false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gather_out", false, Backend::CUDA, ScalarType::Float);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather_out", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather_out", false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gather_out", false, Backend::CUDA, ScalarType::Int);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather_out", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather_out", false, Backend::CUDA, ScalarType::Long);
            THCudaIntTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gather_out", false, Backend::CUDA, ScalarType::Long);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather_out", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gather_out", false, Backend::CUDA, ScalarType::Short);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather_out", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather_out", false, Backend::CUDA, ScalarType::Long);
            THCudaShortTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gather_out", false, Backend::CUDA, ScalarType::Half);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather_out", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather_out", false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gather_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_gather(const Tensor & self, int64_t dim, const Tensor & index) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather", false, Backend::CUDA, ScalarType::Long);
            THCudaBoolTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather", false, Backend::CUDA, ScalarType::Long);
            THCudaByteTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather", false, Backend::CUDA, ScalarType::Long);
            THCudaCharTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather", false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather", false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather", false, Backend::CUDA, ScalarType::Long);
            THCudaIntTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather", false, Backend::CUDA, ScalarType::Long);
            THCudaShortTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gather", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index, "index", 3, "_th_gather", false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gather not supported on CUDAType for ", dispatch_scalar_type);
    }
}
bool _th_equal(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_equal", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_equal", false, Backend::CUDA, ScalarType::Bool);
            return THCudaBoolTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_equal", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_equal", false, Backend::CUDA, ScalarType::Byte);
            return THCudaByteTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_equal", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_equal", false, Backend::CUDA, ScalarType::Char);
            return THCudaCharTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_equal", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_equal", false, Backend::CUDA, ScalarType::Double);
            return THCudaDoubleTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_equal", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_equal", false, Backend::CUDA, ScalarType::Float);
            return THCudaTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_equal", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_equal", false, Backend::CUDA, ScalarType::Int);
            return THCudaIntTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_equal", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_equal", false, Backend::CUDA, ScalarType::Long);
            return THCudaLongTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_equal", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_equal", false, Backend::CUDA, ScalarType::Short);
            return THCudaShortTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_equal", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_equal", false, Backend::CUDA, ScalarType::Half);
            return THCudaHalfTensor_equal(globalContext().getTHCState(), self_, other_);
            break;
        }
        default:
            AT_ERROR("_th_equal not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_and_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_and_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_and(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_and not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_and_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_and_out");
    return s__th_and_out(result, b_self, b_other);
}
Tensor & s__th_and_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_and_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_and_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_and(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_and");
    return s__th_and(b_self, b_other);
}
Tensor s__th_and(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_and", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_and", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_and not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_iand_(Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_iand_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_iand_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_iand_");
    return s__th_iand_(self, b_other);
}
Tensor & s__th_iand_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_iand_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_iand_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_iand_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_iand_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_iand_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_iand_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_iand_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_iand_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_iand_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_iand_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_iand_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_or_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_or(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_or_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_or_out");
    return s__th_or_out(result, b_self, b_other);
}
Tensor & s__th_or_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_or_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_or(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_or");
    return s__th_or(b_self, b_other);
}
Tensor s__th_or(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_or", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_or", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ior_(Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ior_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ior_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_ior_");
    return s__th_ior_(self, b_other);
}
Tensor & s__th_ior_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ior_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ior_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ior_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ior_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ior_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ior_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ior_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ior_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ior_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ior_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ior_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_xor_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_xor_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_xor(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_xor not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_xor_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_xor_out");
    return s__th_xor_out(result, b_self, b_other);
}
Tensor & s__th_xor_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_xor_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_xor_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_xor(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_xor");
    return s__th_xor(b_self, b_other);
}
Tensor s__th_xor(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_xor", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_xor", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_xor not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ixor_(Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ixor_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ixor_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_ixor_");
    return s__th_ixor_(self, b_other);
}
Tensor & s__th_ixor_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ixor_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ixor_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ixor_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ixor_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ixor_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ixor_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ixor_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ixor_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ixor_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ixor_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ixor_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_lshift_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_lshift(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_lshift_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_lshift_out");
    return s__th_lshift_out(result, b_self, b_other);
}
Tensor & s__th_lshift_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_lshift(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_lshift");
    return s__th_lshift(b_self, b_other);
}
Tensor s__th_lshift(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lshift", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lshift", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ilshift_(Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ilshift_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ilshift_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_ilshift_");
    return s__th_ilshift_(self, b_other);
}
Tensor & s__th_ilshift_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ilshift_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_rshift_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_rshift(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_rshift_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_rshift_out");
    return s__th_rshift_out(result, b_self, b_other);
}
Tensor & s__th_rshift_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_rshift(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_rshift");
    return s__th_rshift(b_self, b_other);
}
Tensor s__th_rshift(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_rshift", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_rshift", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_irshift_(Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_irshift_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_irshift_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_irshift_");
    return s__th_irshift_(self, b_other);
}
Tensor & s__th_irshift_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_irshift_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_irshift_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_irshift_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_irshift_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_irshift_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_irshift_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_irshift_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_irshift_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_irshift_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_irshift_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_lt_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_lt(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_lt_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_lt_out");
    return s__th_lt_out(result, b_self, b_other);
}
Tensor & s__th_lt_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_lt(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_lt");
    return s__th_lt(b_self, b_other);
}
Tensor s__th_lt(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_lt_byte_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt_byte_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_lt_byte(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_ltValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt_byte not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_lt_byte_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_lt_byte_out");
    return s__th_lt_byte_out(result, b_self, b_other);
}
Tensor & s__th_lt_byte_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt_byte_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_lt_byte(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_lt_byte");
    return s__th_lt_byte(b_self, b_other);
}
Tensor s__th_lt_byte(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_byte", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_lt_byte", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_ltTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt_byte not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_lt_(Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_lt_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_lt_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_lt_");
    return s__th_lt_(self, b_other);
}
Tensor & s__th_lt_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_lt_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_lt_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_lt_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_lt_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_lt_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_lt_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_lt_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_lt_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_lt_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_lt_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_lt_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_gt_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_gt(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_gt_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_gt_out");
    return s__th_gt_out(result, b_self, b_other);
}
Tensor & s__th_gt_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_gt(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_gt");
    return s__th_gt(b_self, b_other);
}
Tensor s__th_gt(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_gt_byte_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt_byte_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_gt_byte(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_gtValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt_byte not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_gt_byte_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_gt_byte_out");
    return s__th_gt_byte_out(result, b_self, b_other);
}
Tensor & s__th_gt_byte_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt_byte_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_gt_byte(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_gt_byte");
    return s__th_gt_byte(b_self, b_other);
}
Tensor s__th_gt_byte(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_byte", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_gt_byte", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_gtTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt_byte not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_gt_(Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_gt_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_gt_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_gt_");
    return s__th_gt_(self, b_other);
}
Tensor & s__th_gt_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_gt_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_gt_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_gt_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_gt_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_gt_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_gt_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_gt_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_gt_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gt_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_gt_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_gt_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_le_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_le(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_le_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_le_out");
    return s__th_le_out(result, b_self, b_other);
}
Tensor & s__th_le_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_le(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_le");
    return s__th_le(b_self, b_other);
}
Tensor s__th_le(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_le_byte_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le_byte_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_le_byte(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_leValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le_byte not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_le_byte_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_le_byte_out");
    return s__th_le_byte_out(result, b_self, b_other);
}
Tensor & s__th_le_byte_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le_byte_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_le_byte(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_le_byte");
    return s__th_le_byte(b_self, b_other);
}
Tensor s__th_le_byte(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_byte", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_le_byte", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_leTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le_byte not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_le_(Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_le_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_le_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_le_");
    return s__th_le_(self, b_other);
}
Tensor & s__th_le_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_le_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_le_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_le_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_le_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_le_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_le_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_le_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_le_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_le_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_le_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_le_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ge_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_ge(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ge_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_ge_out");
    return s__th_ge_out(result, b_self, b_other);
}
Tensor & s__th_ge_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_ge(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_ge");
    return s__th_ge(b_self, b_other);
}
Tensor s__th_ge(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ge_byte_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge_byte_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_ge_byte(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_geValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge_byte not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ge_byte_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_ge_byte_out");
    return s__th_ge_byte_out(result, b_self, b_other);
}
Tensor & s__th_ge_byte_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge_byte_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_ge_byte(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_ge_byte");
    return s__th_ge_byte(b_self, b_other);
}
Tensor s__th_ge_byte(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_byte", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ge_byte", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_geTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge_byte not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ge_(Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ge_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ge_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_ge_");
    return s__th_ge_(self, b_other);
}
Tensor & s__th_ge_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ge_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ge_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ge_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ge_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ge_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ge_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ge_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ge_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ge_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ge_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ge_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_eq_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_eq(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_eq_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_eq_out");
    return s__th_eq_out(result, b_self, b_other);
}
Tensor & s__th_eq_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_eq(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_eq");
    return s__th_eq(b_self, b_other);
}
Tensor s__th_eq(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_eq_byte_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq_byte_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_eq_byte(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_eqValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq_byte not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_eq_byte_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_eq_byte_out");
    return s__th_eq_byte_out(result, b_self, b_other);
}
Tensor & s__th_eq_byte_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq_byte_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_eq_byte(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_eq_byte");
    return s__th_eq_byte(b_self, b_other);
}
Tensor s__th_eq_byte(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_byte", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_eq_byte", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_eqTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq_byte not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_eq_(Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_eq_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_eq_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_eq_");
    return s__th_eq_(self, b_other);
}
Tensor & s__th_eq_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_eq_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_eq_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_eq_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_eq_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_eq_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_eq_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_eq_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_eq_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eq_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_eq_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_eq_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ne_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_ne(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ne_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_ne_out");
    return s__th_ne_out(result, b_self, b_other);
}
Tensor & s__th_ne_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_ne(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_ne");
    return s__th_ne(b_self, b_other);
}
Tensor s__th_ne(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Bool), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ne_byte_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne_byte_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_ne_byte(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_neValueByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne_byte not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ne_byte_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_ne_byte_out");
    return s__th_ne_byte_out(result, b_self, b_other);
}
Tensor & s__th_ne_byte_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne_byte_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_ne_byte(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_ne_byte");
    return s__th_ne_byte(b_self, b_other);
}
Tensor s__th_ne_byte(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_byte", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_ne_byte", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_neTensorByte(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne_byte not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ne_(Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = other.toBool();
            THCudaBoolTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ne_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ne_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_ne_");
    return s__th_ne_(self, b_other);
}
Tensor & s__th_ne_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ne_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ne_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ne_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ne_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ne_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ne_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ne_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ne_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ne_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_ne_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ne_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_min_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_min_out");
    return s__th_min_out(result, b_self, b_other);
}
Tensor & s__th_min_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_min_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_min(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_min");
    return s__th_min(b_self, b_other);
}
Tensor s__th_min(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_min", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_min not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_min(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Bool);
            return at::scalar_tensor(convert<bool>(THCudaBoolTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Bool));
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Byte);
            return at::scalar_tensor(convert<uint8_t>(THCudaByteTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Byte));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Char);
            return at::scalar_tensor(convert<int8_t>(THCudaCharTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Char));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THCudaTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Int);
            return at::scalar_tensor(convert<int>(THCudaIntTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Int));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THCudaLongTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Long));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Short);
            return at::scalar_tensor(convert<int16_t>(THCudaShortTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Short));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Half);
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_minall(globalContext().getTHCState(), self_)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_min not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_min_out(Tensor & min, Tensor & min_indices, const Tensor & self, int64_t dim, bool keepdim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto min_ = checked_tensor_unwrap(min, "min", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Bool);
            auto min_indices_ = checked_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THCudaBoolTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Byte: {
            auto min_ = checked_tensor_unwrap(min, "min", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Byte);
            auto min_indices_ = checked_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Char: {
            auto min_ = checked_tensor_unwrap(min, "min", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Char);
            auto min_indices_ = checked_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Double: {
            auto min_ = checked_tensor_unwrap(min, "min", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Double);
            auto min_indices_ = checked_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Float: {
            auto min_ = checked_tensor_unwrap(min, "min", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Float);
            auto min_indices_ = checked_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Int: {
            auto min_ = checked_tensor_unwrap(min, "min", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Int);
            auto min_indices_ = checked_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Long: {
            auto min_ = checked_tensor_unwrap(min, "min", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            auto min_indices_ = checked_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Short: {
            auto min_ = checked_tensor_unwrap(min, "min", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Short);
            auto min_indices_ = checked_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Half: {
            auto min_ = checked_tensor_unwrap(min, "min", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Half);
            auto min_indices_ = checked_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min_out", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        default:
            AT_ERROR("_th_min_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_min(const Tensor & self, int64_t dim, bool keepdim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THCudaBoolTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Byte: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Char: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Double: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Float: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Int: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Long: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Short: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Half: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_min", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        default:
            AT_ERROR("_th_min not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_max_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_max_out");
    return s__th_max_out(result, b_self, b_other);
}
Tensor & s__th_max_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max_out", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_max_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_max(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_max");
    return s__th_max(b_self, b_other);
}
Tensor s__th_max(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Bool);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_max", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_max not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_max(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Bool);
            return at::scalar_tensor(convert<bool>(THCudaBoolTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Bool));
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Byte);
            return at::scalar_tensor(convert<uint8_t>(THCudaByteTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Byte));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Char);
            return at::scalar_tensor(convert<int8_t>(THCudaCharTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Char));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THCudaTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Int);
            return at::scalar_tensor(convert<int>(THCudaIntTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Int));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THCudaLongTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Long));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Short);
            return at::scalar_tensor(convert<int16_t>(THCudaShortTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Short));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Half);
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_maxall(globalContext().getTHCState(), self_)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_max not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_max_out(Tensor & max, Tensor & max_indices, const Tensor & self, int64_t dim, bool keepdim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto max_ = checked_tensor_unwrap(max, "max", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Bool);
            auto max_indices_ = checked_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THCudaBoolTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Byte: {
            auto max_ = checked_tensor_unwrap(max, "max", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Byte);
            auto max_indices_ = checked_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Char: {
            auto max_ = checked_tensor_unwrap(max, "max", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Char);
            auto max_indices_ = checked_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Double: {
            auto max_ = checked_tensor_unwrap(max, "max", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Double);
            auto max_indices_ = checked_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Float: {
            auto max_ = checked_tensor_unwrap(max, "max", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Float);
            auto max_indices_ = checked_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Int: {
            auto max_ = checked_tensor_unwrap(max, "max", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Int);
            auto max_indices_ = checked_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Long: {
            auto max_ = checked_tensor_unwrap(max, "max", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            auto max_indices_ = checked_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Short: {
            auto max_ = checked_tensor_unwrap(max, "max", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Short);
            auto max_indices_ = checked_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Half: {
            auto max_ = checked_tensor_unwrap(max, "max", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Half);
            auto max_indices_ = checked_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max_out", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        default:
            AT_ERROR("_th_max_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_max(const Tensor & self, int64_t dim, bool keepdim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THCudaBoolTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Byte: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Char: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Double: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Float: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Int: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Long: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Short: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Half: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_max", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        default:
            AT_ERROR("_th_max not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_mode_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Byte);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode_out", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Char);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode_out", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode_out", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode_out", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Int);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode_out", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Long);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode_out", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Short);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode_out", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Half: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode_out", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_mode_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_mode(const Tensor & self, int64_t dim, bool keepdim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Half: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mode", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_mode not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_sort_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool descending) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Byte);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort_out", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Char);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort_out", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort_out", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort_out", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Int);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort_out", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Long);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort_out", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Short);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort_out", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Half: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort_out", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_sort_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_sort(const Tensor & self, int64_t dim, bool descending) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Half: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sort", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_sort not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_topk_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Byte);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk_out", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Char);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk_out", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Double);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk_out", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk_out", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Int);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk_out", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Long);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk_out", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Short);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk_out", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Half: {
            auto values_ = checked_tensor_unwrap(values, "values", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Half);
            auto indices_ = checked_tensor_unwrap(indices, "indices", 0, "_th_topk_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk_out", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_topk_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_topk(const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Half: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_topk", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_topk not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_abs_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_abs_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_abs_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_abs_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_abs_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_abs_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_abs_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_abs_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_abs_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_abs_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_abs(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_abs", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_abs(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_abs not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_sigmoid_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_sigmoid_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sigmoid_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sigmoid(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_sigmoid_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sigmoid_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sigmoid(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_sigmoid_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sigmoid_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sigmoid(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sigmoid_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_sigmoid(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sigmoid", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sigmoid(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sigmoid", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sigmoid(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sigmoid", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sigmoid(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sigmoid not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_log_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_log_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_log_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_log_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_log(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_log10_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_log10_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log10_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log10(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_log10_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log10_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log10(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_log10_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log10_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log10(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log10_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_log10(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log10", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log10(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log10", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log10(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log10", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log10(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log10 not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_log1p_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_log1p_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log1p_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log1p(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_log1p_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log1p_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log1p(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_log1p_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log1p_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log1p(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log1p_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_log1p(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log1p", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log1p(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log1p", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log1p(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log1p", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log1p(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log1p not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_log2_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_log2_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log2_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log2(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_log2_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log2_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log2(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_log2_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log2_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log2(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log2_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_log2(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log2", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_log2(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log2", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_log2(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_log2", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_log2(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_log2 not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_exp_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_exp_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_exp_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_exp(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_exp_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_exp_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_exp(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_exp_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_exp_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_exp(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_exp_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_exp(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_exp", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_exp(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_exp", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_exp(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_exp", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_exp(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_exp not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_expm1_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_expm1_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_expm1_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_expm1(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_expm1_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_expm1_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_expm1(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_expm1_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_expm1_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_expm1(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_expm1_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_expm1(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_expm1", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_expm1(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_expm1", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_expm1(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_expm1", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_expm1(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_expm1 not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_cos_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cos_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cos_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cos_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cos_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cos_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cos_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cos_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_cos(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cos", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cos", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cos", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cos not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_acos_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_acos_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_acos_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_acos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_acos_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_acos_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_acos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_acos_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_acos_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_acos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_acos_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_acos(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_acos", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_acos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_acos", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_acos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_acos", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_acos(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_acos not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_cosh_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cosh_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cosh_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cosh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cosh_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cosh_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cosh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cosh_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cosh_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cosh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cosh_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_cosh(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cosh", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cosh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cosh", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cosh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cosh", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cosh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cosh not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_sin_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_sin_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sin_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_sin_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sin_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_sin_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sin_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sin_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_sin(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sin", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sin", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sin", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sin not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_asin_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_asin_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_asin_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_asin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_asin_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_asin_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_asin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_asin_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_asin_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_asin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_asin_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_asin(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_asin", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_asin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_asin", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_asin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_asin", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_asin(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_asin not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_sinh_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_sinh_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sinh_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sinh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_sinh_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sinh_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sinh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_sinh_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sinh_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sinh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sinh_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_sinh(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sinh", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sinh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sinh", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sinh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sinh", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sinh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sinh not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_tan_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_tan_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_tan_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_tan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_tan_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_tan_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_tan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_tan_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_tan_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_tan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_tan_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_tan(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_tan", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_tan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_tan", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_tan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_tan", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_tan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_tan not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_atan_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_atan_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_atan_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_atan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_atan_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_atan_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_atan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_atan_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_atan_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_atan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_atan_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_atan(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_atan", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_atan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_atan", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_atan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_atan", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_atan(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_atan not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_tanh_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_tanh_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_tanh_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_tanh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_tanh_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_tanh_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_tanh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_tanh_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_tanh_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_tanh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_tanh_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_tanh(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_tanh", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_tanh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_tanh", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_tanh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_tanh", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_tanh(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_tanh not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_erf_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_erf_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_erf_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_erf(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_erf_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_erf_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_erf(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_erf_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_erf_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_erf(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_erf_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_erf(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_erf", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_erf(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_erf", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_erf(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_erf", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_erf(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_erf not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_erfc_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_erfc_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_erfc_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_erfc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_erfc_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_erfc_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_erfc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_erfc_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_erfc_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_erfc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_erfc_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_erfc(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_erfc", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_erfc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_erfc", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_erfc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_erfc", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_erfc(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_erfc not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_sqrt_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_sqrt_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sqrt_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_sqrt_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sqrt_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_sqrt_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sqrt_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sqrt_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_sqrt(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sqrt", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_sqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sqrt", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_sqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_sqrt", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_sqrt(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sqrt not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_frac_(Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_frac_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_frac(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_frac_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_frac(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_frac_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_frac(globalContext().getTHCState(), self_, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_frac_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_frac_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_frac_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_frac_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_frac(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_frac_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_frac_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_frac(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_frac_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_frac_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_frac(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_frac_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_frac(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_frac", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_frac(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_frac", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_frac(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_frac", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_frac(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_frac not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_var_out(Tensor & result, const Tensor & self, int64_t dim, bool unbiased, bool keepdim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_var_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_var_out", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_var_single(globalContext().getTHCState(), result_, self_, dim, unbiased, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_var_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_var_out", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_var_single(globalContext().getTHCState(), result_, self_, dim, unbiased, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_var_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_var_out", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_var_single(globalContext().getTHCState(), result_, self_, dim, unbiased, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_var_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_var(const Tensor & self, int64_t dim, bool unbiased, bool keepdim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_var", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_var_single(globalContext().getTHCState(), result_, self_, dim, unbiased, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_var", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_var_single(globalContext().getTHCState(), result_, self_, dim, unbiased, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_var", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_var_single(globalContext().getTHCState(), result_, self_, dim, unbiased, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_var not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_var(const Tensor & self, bool unbiased) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_var", false, Backend::CUDA, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_var_all(globalContext().getTHCState(), self_, unbiased)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_var", false, Backend::CUDA, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THCudaTensor_var_all(globalContext().getTHCState(), self_, unbiased)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_var", false, Backend::CUDA, ScalarType::Half);
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_var_all(globalContext().getTHCState(), self_, unbiased)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_var not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_std_out(Tensor & result, const Tensor & self, int64_t dim, bool unbiased, bool keepdim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_std_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_std_out", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_std_single(globalContext().getTHCState(), result_, self_, dim, unbiased, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_std_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_std_out", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_std_single(globalContext().getTHCState(), result_, self_, dim, unbiased, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_std_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_std_out", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_std_single(globalContext().getTHCState(), result_, self_, dim, unbiased, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_std_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_std(const Tensor & self, int64_t dim, bool unbiased, bool keepdim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_std", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_std_single(globalContext().getTHCState(), result_, self_, dim, unbiased, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_std", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_std_single(globalContext().getTHCState(), result_, self_, dim, unbiased, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_std", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_std_single(globalContext().getTHCState(), result_, self_, dim, unbiased, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_std not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_std(const Tensor & self, bool unbiased) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_std", false, Backend::CUDA, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_std_all(globalContext().getTHCState(), self_, unbiased)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_std", false, Backend::CUDA, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THCudaTensor_std_all(globalContext().getTHCState(), self_, unbiased)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_std", false, Backend::CUDA, ScalarType::Half);
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_std_all(globalContext().getTHCState(), self_, unbiased)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_std not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_renorm_out(Tensor & result, const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_renorm_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_renorm_out", false, Backend::CUDA, ScalarType::Double);
            auto p_ = p.toDouble();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toDouble();
            THCudaDoubleTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_renorm_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_renorm_out", false, Backend::CUDA, ScalarType::Float);
            auto p_ = p.toFloat();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toFloat();
            THCudaTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_renorm_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_renorm_out", false, Backend::CUDA, ScalarType::Half);
            auto p_ = p.toHalf();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toHalf();
            THCudaHalfTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_renorm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_renorm(const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_renorm", false, Backend::CUDA, ScalarType::Double);
            auto p_ = p.toDouble();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toDouble();
            THCudaDoubleTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_renorm", false, Backend::CUDA, ScalarType::Float);
            auto p_ = p.toFloat();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toFloat();
            THCudaTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_renorm", false, Backend::CUDA, ScalarType::Half);
            auto p_ = p.toHalf();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toHalf();
            THCudaHalfTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_renorm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_renorm_(Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_renorm_", false, Backend::CUDA, ScalarType::Double);
            auto p_ = p.toDouble();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toDouble();
            THCudaDoubleTensor_renorm(globalContext().getTHCState(), self_, self_, p_, dim, maxnorm_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_renorm_", false, Backend::CUDA, ScalarType::Float);
            auto p_ = p.toFloat();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toFloat();
            THCudaTensor_renorm(globalContext().getTHCState(), self_, self_, p_, dim, maxnorm_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_renorm_", false, Backend::CUDA, ScalarType::Half);
            auto p_ = p.toHalf();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toHalf();
            THCudaHalfTensor_renorm(globalContext().getTHCState(), self_, self_, p_, dim, maxnorm_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_renorm_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_dist(const Tensor & self, const Tensor & other, Scalar p) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_dist");
    return s__th_dist(b_self, b_other, p);
}
Tensor s__th_dist(const Tensor & self, const Tensor & other, Scalar p) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_dist", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_dist", false, Backend::CUDA, ScalarType::Double);
            auto p_ = p.toDouble();
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_dist(globalContext().getTHCState(), self_, other_, p_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_dist", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_dist", false, Backend::CUDA, ScalarType::Float);
            auto p_ = p.toFloat();
            return at::scalar_tensor(convert<float>(THCudaTensor_dist(globalContext().getTHCState(), self_, other_, p_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_dist", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_dist", false, Backend::CUDA, ScalarType::Half);
            auto p_ = p.toHalf();
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_dist(globalContext().getTHCState(), self_, other_, p_)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_dist not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_reciprocal_out(Tensor & result, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_reciprocal_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_reciprocal_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_reciprocal_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_reciprocal_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_reciprocal_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_reciprocal_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_reciprocal_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_reciprocal(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_reciprocal", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_reciprocal", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_reciprocal", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cinv(globalContext().getTHCState(), result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_reciprocal not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_zero_(Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_zero_", false, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_zero_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_zero_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_zero_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_zero_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_zero_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_zero_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_zero_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_zero_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_zero(globalContext().getTHCState(), self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_zero_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_cumsum_out(Tensor & result, const Tensor & self, int64_t dim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THCudaBoolTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumsum_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_cumsum(const Tensor & self, int64_t dim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THCudaBoolTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumsum", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumsum not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_cumprod_out(Tensor & result, const Tensor & self, int64_t dim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Bool);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THCudaBoolTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumprod_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_cumprod(const Tensor & self, int64_t dim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod", false, Backend::CUDA, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THCudaBoolTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod", false, Backend::CUDA, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THCudaByteTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod", false, Backend::CUDA, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCudaCharTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THCudaDoubleTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THCudaTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod", false, Backend::CUDA, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THCudaIntTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod", false, Backend::CUDA, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THCudaLongTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod", false, Backend::CUDA, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THCudaShortTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cumprod", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            THCudaHalfTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumprod not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_trace(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_trace", false, Backend::CUDA, ScalarType::Byte);
            return at::scalar_tensor(convert<uint8_t>(THCudaByteTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Byte));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_trace", false, Backend::CUDA, ScalarType::Char);
            return at::scalar_tensor(convert<int8_t>(THCudaCharTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Char));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_trace", false, Backend::CUDA, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_trace", false, Backend::CUDA, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THCudaTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_trace", false, Backend::CUDA, ScalarType::Int);
            return at::scalar_tensor(convert<int>(THCudaIntTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Int));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_trace", false, Backend::CUDA, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THCudaLongTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Long));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_trace", false, Backend::CUDA, ScalarType::Short);
            return at::scalar_tensor(convert<int16_t>(THCudaShortTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Short));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_trace", false, Backend::CUDA, ScalarType::Half);
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_trace(globalContext().getTHCState(), self_)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_trace not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_fmod_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_fmod(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_fmod_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_fmod_out");
    return s__th_fmod_out(result, b_self, b_other);
}
Tensor & s__th_fmod_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_fmod(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_fmod");
    return s__th_fmod(b_self, b_other);
}
Tensor s__th_fmod(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_fmod", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_fmod_(Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_fmod_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_fmod_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_fmod_");
    return s__th_fmod_(self, b_other);
}
Tensor & s__th_fmod_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_fmod_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_fmod_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_fmod_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_fmod_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_fmod_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_fmod_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_fmod_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_fmod_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_fmod_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_fmod_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_remainder_out(Tensor & result, const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_remainder(const Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_remainder_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_remainder_out");
    return s__th_remainder_out(result, b_self, b_other);
}
Tensor & s__th_remainder_out(Tensor & result, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_remainder(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_remainder");
    return s__th_remainder(b_self, b_other);
}
Tensor s__th_remainder(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_remainder", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_remainder_(Tensor & self, Scalar other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = other.toByte();
            THCudaByteTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = other.toChar();
            THCudaCharTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = other.toDouble();
            THCudaDoubleTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = other.toFloat();
            THCudaTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = other.toInt();
            THCudaIntTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = other.toLong();
            THCudaLongTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = other.toShort();
            THCudaShortTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = other.toHalf();
            THCudaHalfTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_remainder_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_remainder_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_remainder_");
    return s__th_remainder_(self, b_other);
}
Tensor & s__th_remainder_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_remainder_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_remainder_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_remainder_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_remainder_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_remainder_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_remainder_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_remainder_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_remainder_", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 3, "_th_remainder_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_remainder_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_clamp_out(Tensor & result, const Tensor & self, Scalar min, Scalar max) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_out", false, Backend::CUDA, ScalarType::Byte);
            auto min_ = min.toByte();
            auto max_ = max.toByte();
            THCudaByteTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_out", false, Backend::CUDA, ScalarType::Char);
            auto min_ = min.toChar();
            auto max_ = max.toChar();
            THCudaCharTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_out", false, Backend::CUDA, ScalarType::Double);
            auto min_ = min.toDouble();
            auto max_ = max.toDouble();
            THCudaDoubleTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_out", false, Backend::CUDA, ScalarType::Float);
            auto min_ = min.toFloat();
            auto max_ = max.toFloat();
            THCudaTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_out", false, Backend::CUDA, ScalarType::Int);
            auto min_ = min.toInt();
            auto max_ = max.toInt();
            THCudaIntTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_out", false, Backend::CUDA, ScalarType::Long);
            auto min_ = min.toLong();
            auto max_ = max.toLong();
            THCudaLongTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_out", false, Backend::CUDA, ScalarType::Short);
            auto min_ = min.toShort();
            auto max_ = max.toShort();
            THCudaShortTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_out", false, Backend::CUDA, ScalarType::Half);
            auto min_ = min.toHalf();
            auto max_ = max.toHalf();
            THCudaHalfTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_clamp(const Tensor & self, Scalar min, Scalar max) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp", false, Backend::CUDA, ScalarType::Byte);
            auto min_ = min.toByte();
            auto max_ = max.toByte();
            THCudaByteTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp", false, Backend::CUDA, ScalarType::Char);
            auto min_ = min.toChar();
            auto max_ = max.toChar();
            THCudaCharTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp", false, Backend::CUDA, ScalarType::Double);
            auto min_ = min.toDouble();
            auto max_ = max.toDouble();
            THCudaDoubleTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp", false, Backend::CUDA, ScalarType::Float);
            auto min_ = min.toFloat();
            auto max_ = max.toFloat();
            THCudaTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp", false, Backend::CUDA, ScalarType::Int);
            auto min_ = min.toInt();
            auto max_ = max.toInt();
            THCudaIntTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp", false, Backend::CUDA, ScalarType::Long);
            auto min_ = min.toLong();
            auto max_ = max.toLong();
            THCudaLongTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp", false, Backend::CUDA, ScalarType::Short);
            auto min_ = min.toShort();
            auto max_ = max.toShort();
            THCudaShortTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp", false, Backend::CUDA, ScalarType::Half);
            auto min_ = min.toHalf();
            auto max_ = max.toHalf();
            THCudaHalfTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_clamp_min_out(Tensor & result, const Tensor & self, Scalar min) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Byte);
            auto min_ = min.toByte();
            THCudaByteTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Char);
            auto min_ = min.toChar();
            THCudaCharTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Double);
            auto min_ = min.toDouble();
            THCudaDoubleTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Float);
            auto min_ = min.toFloat();
            THCudaTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Int);
            auto min_ = min.toInt();
            THCudaIntTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Long);
            auto min_ = min.toLong();
            THCudaLongTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Short);
            auto min_ = min.toShort();
            THCudaShortTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min_out", false, Backend::CUDA, ScalarType::Half);
            auto min_ = min.toHalf();
            THCudaHalfTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_min_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_clamp_min(const Tensor & self, Scalar min) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min", false, Backend::CUDA, ScalarType::Byte);
            auto min_ = min.toByte();
            THCudaByteTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min", false, Backend::CUDA, ScalarType::Char);
            auto min_ = min.toChar();
            THCudaCharTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min", false, Backend::CUDA, ScalarType::Double);
            auto min_ = min.toDouble();
            THCudaDoubleTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min", false, Backend::CUDA, ScalarType::Float);
            auto min_ = min.toFloat();
            THCudaTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min", false, Backend::CUDA, ScalarType::Int);
            auto min_ = min.toInt();
            THCudaIntTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min", false, Backend::CUDA, ScalarType::Long);
            auto min_ = min.toLong();
            THCudaLongTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min", false, Backend::CUDA, ScalarType::Short);
            auto min_ = min.toShort();
            THCudaShortTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_min", false, Backend::CUDA, ScalarType::Half);
            auto min_ = min.toHalf();
            THCudaHalfTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_min not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_clamp_max_out(Tensor & result, const Tensor & self, Scalar max) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Byte);
            auto max_ = max.toByte();
            THCudaByteTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Char);
            auto max_ = max.toChar();
            THCudaCharTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Double);
            auto max_ = max.toDouble();
            THCudaDoubleTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Float);
            auto max_ = max.toFloat();
            THCudaTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Int);
            auto max_ = max.toInt();
            THCudaIntTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Long);
            auto max_ = max.toLong();
            THCudaLongTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Short);
            auto max_ = max.toShort();
            THCudaShortTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max_out", false, Backend::CUDA, ScalarType::Half);
            auto max_ = max.toHalf();
            THCudaHalfTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_max_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_clamp_max(const Tensor & self, Scalar max) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max", false, Backend::CUDA, ScalarType::Byte);
            auto max_ = max.toByte();
            THCudaByteTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max", false, Backend::CUDA, ScalarType::Char);
            auto max_ = max.toChar();
            THCudaCharTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max", false, Backend::CUDA, ScalarType::Double);
            auto max_ = max.toDouble();
            THCudaDoubleTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max", false, Backend::CUDA, ScalarType::Float);
            auto max_ = max.toFloat();
            THCudaTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max", false, Backend::CUDA, ScalarType::Int);
            auto max_ = max.toInt();
            THCudaIntTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max", false, Backend::CUDA, ScalarType::Long);
            auto max_ = max.toLong();
            THCudaLongTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max", false, Backend::CUDA, ScalarType::Short);
            auto max_ = max.toShort();
            THCudaShortTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_clamp_max", false, Backend::CUDA, ScalarType::Half);
            auto max_ = max.toHalf();
            THCudaHalfTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_max not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_dot(const Tensor & self, const Tensor & tensor) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_dot", false, Backend::CUDA, ScalarType::Double);
            auto tensor_ = checked_tensor_unwrap(tensor, "tensor", 2, "_th_dot", false, Backend::CUDA, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THCudaDoubleTensor_dot(globalContext().getTHCState(), self_, tensor_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_dot", false, Backend::CUDA, ScalarType::Float);
            auto tensor_ = checked_tensor_unwrap(tensor, "tensor", 2, "_th_dot", false, Backend::CUDA, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THCudaTensor_dot(globalContext().getTHCState(), self_, tensor_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_dot", false, Backend::CUDA, ScalarType::Half);
            auto tensor_ = checked_tensor_unwrap(tensor, "tensor", 2, "_th_dot", false, Backend::CUDA, ScalarType::Half);
            return at::scalar_tensor(convert<Half>(THCudaHalfTensor_dot(globalContext().getTHCState(), self_, tensor_)), options(ScalarType::Half));
            break;
        }
        default:
            AT_ERROR("_th_dot not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_cross_kernel_out(Tensor & result, const Tensor & self, const Tensor & other, int64_t dim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cross_kernel_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_cross_kernel(const Tensor & self, const Tensor & other, int64_t dim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Byte);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Char);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Double);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Int);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Short);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Half);
            auto other_ = checked_tensor_unwrap(other, "other", 2, "_th_cross_kernel", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_crossKernel(globalContext().getTHCState(), result_, self_, other_, dim);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cross_kernel not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_diag_out(Tensor & result, const Tensor & self, int64_t diagonal) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_diag_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag_out", false, Backend::CUDA, ScalarType::Byte);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaByteTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_diag_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag_out", false, Backend::CUDA, ScalarType::Char);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaCharTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_diag_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag_out", false, Backend::CUDA, ScalarType::Double);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaDoubleTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_diag_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag_out", false, Backend::CUDA, ScalarType::Float);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_diag_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag_out", false, Backend::CUDA, ScalarType::Int);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaIntTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_diag_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag_out", false, Backend::CUDA, ScalarType::Long);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaLongTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_diag_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag_out", false, Backend::CUDA, ScalarType::Short);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaShortTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_diag_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag_out", false, Backend::CUDA, ScalarType::Half);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaHalfTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_diag_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_diag(const Tensor & self, int64_t diagonal) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag", false, Backend::CUDA, ScalarType::Byte);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaByteTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag", false, Backend::CUDA, ScalarType::Char);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaCharTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag", false, Backend::CUDA, ScalarType::Double);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaDoubleTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag", false, Backend::CUDA, ScalarType::Float);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag", false, Backend::CUDA, ScalarType::Int);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaIntTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag", false, Backend::CUDA, ScalarType::Long);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaLongTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag", false, Backend::CUDA, ScalarType::Short);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaShortTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_diag", false, Backend::CUDA, ScalarType::Half);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THCudaHalfTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_diag not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {mat1.size(0),mat2.size(1)}, "_th_addmm_out");
    return s__th_addmm_out(result, b_self, mat1, mat2, beta, alpha);
}
Tensor & s__th_addmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, Backend::CUDA, ScalarType::Byte);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, Backend::CUDA, ScalarType::Char);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, Backend::CUDA, ScalarType::Double);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, Backend::CUDA, ScalarType::Float);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, Backend::CUDA, ScalarType::Int);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, Backend::CUDA, ScalarType::Long);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, Backend::CUDA, ScalarType::Short);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, Backend::CUDA, ScalarType::Half);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {mat1.size(0),mat2.size(1)}, "_th_addmm");
    return s__th_addmm(b_self, mat1, mat2, beta, alpha);
}
Tensor s__th_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm", false, Backend::CUDA, ScalarType::Byte);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm", false, Backend::CUDA, ScalarType::Char);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm", false, Backend::CUDA, ScalarType::Double);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm", false, Backend::CUDA, ScalarType::Float);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm", false, Backend::CUDA, ScalarType::Int);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm", false, Backend::CUDA, ScalarType::Long);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm", false, Backend::CUDA, ScalarType::Short);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm", false, Backend::CUDA, ScalarType::Half);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_addmm(globalContext().getTHCState(), result_, self_, mat1_, mat2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_", false, Backend::CUDA, ScalarType::Byte);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_addmm(globalContext().getTHCState(), self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_", false, Backend::CUDA, ScalarType::Char);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_addmm(globalContext().getTHCState(), self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_", false, Backend::CUDA, ScalarType::Double);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_addmm(globalContext().getTHCState(), self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_", false, Backend::CUDA, ScalarType::Float);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_addmm(globalContext().getTHCState(), self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_", false, Backend::CUDA, ScalarType::Int);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_addmm(globalContext().getTHCState(), self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_", false, Backend::CUDA, ScalarType::Long);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_addmm(globalContext().getTHCState(), self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_", false, Backend::CUDA, ScalarType::Short);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_addmm(globalContext().getTHCState(), self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmm_", false, Backend::CUDA, ScalarType::Half);
            auto mat1_ = checked_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_addmm(globalContext().getTHCState(), self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addmm_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addmv_out(Tensor & result, const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {mat.size(0)}, "_th_addmv_out");
    return s__th_addmv_out(result, b_self, mat, vec, beta, alpha);
}
Tensor & s__th_addmv_out(Tensor & result, const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, Backend::CUDA, ScalarType::Byte);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, Backend::CUDA, ScalarType::Byte);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, Backend::CUDA, ScalarType::Char);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, Backend::CUDA, ScalarType::Char);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, Backend::CUDA, ScalarType::Double);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, Backend::CUDA, ScalarType::Double);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, Backend::CUDA, ScalarType::Float);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, Backend::CUDA, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, Backend::CUDA, ScalarType::Int);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, Backend::CUDA, ScalarType::Int);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, Backend::CUDA, ScalarType::Long);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, Backend::CUDA, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, Backend::CUDA, ScalarType::Short);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, Backend::CUDA, ScalarType::Short);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, Backend::CUDA, ScalarType::Half);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, Backend::CUDA, ScalarType::Half);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmv_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_addmv(const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {mat.size(0)}, "_th_addmv");
    return s__th_addmv(b_self, mat, vec, beta, alpha);
}
Tensor s__th_addmv(const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv", false, Backend::CUDA, ScalarType::Byte);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, Backend::CUDA, ScalarType::Byte);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv", false, Backend::CUDA, ScalarType::Char);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, Backend::CUDA, ScalarType::Char);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv", false, Backend::CUDA, ScalarType::Double);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, Backend::CUDA, ScalarType::Double);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv", false, Backend::CUDA, ScalarType::Float);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, Backend::CUDA, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv", false, Backend::CUDA, ScalarType::Int);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, Backend::CUDA, ScalarType::Int);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv", false, Backend::CUDA, ScalarType::Long);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, Backend::CUDA, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv", false, Backend::CUDA, ScalarType::Short);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, Backend::CUDA, ScalarType::Short);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv", false, Backend::CUDA, ScalarType::Half);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, Backend::CUDA, ScalarType::Half);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_addmv(globalContext().getTHCState(), result_, self_, mat_, vec_, beta_, alpha_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmv not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addmv_(Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_", false, Backend::CUDA, ScalarType::Byte);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, Backend::CUDA, ScalarType::Byte);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_addmv(globalContext().getTHCState(), self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_", false, Backend::CUDA, ScalarType::Char);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, Backend::CUDA, ScalarType::Char);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_addmv(globalContext().getTHCState(), self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_", false, Backend::CUDA, ScalarType::Double);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, Backend::CUDA, ScalarType::Double);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_addmv(globalContext().getTHCState(), self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_", false, Backend::CUDA, ScalarType::Float);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, Backend::CUDA, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_addmv(globalContext().getTHCState(), self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_", false, Backend::CUDA, ScalarType::Int);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, Backend::CUDA, ScalarType::Int);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_addmv(globalContext().getTHCState(), self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_", false, Backend::CUDA, ScalarType::Long);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, Backend::CUDA, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_addmv(globalContext().getTHCState(), self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_", false, Backend::CUDA, ScalarType::Short);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, Backend::CUDA, ScalarType::Short);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_addmv(globalContext().getTHCState(), self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addmv_", false, Backend::CUDA, ScalarType::Half);
            auto mat_ = checked_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, Backend::CUDA, ScalarType::Half);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_addmv(globalContext().getTHCState(), self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addmv_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addr_out(Tensor & result, const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {vec1.size(0),vec2.size(0)}, "_th_addr_out");
    return s__th_addr_out(result, b_self, vec1, vec2, beta, alpha);
}
Tensor & s__th_addr_out(Tensor & result, const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addr_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_out", false, Backend::CUDA, ScalarType::Byte);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, Backend::CUDA, ScalarType::Byte);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addr_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_out", false, Backend::CUDA, ScalarType::Char);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, Backend::CUDA, ScalarType::Char);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addr_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_out", false, Backend::CUDA, ScalarType::Double);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, Backend::CUDA, ScalarType::Double);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addr_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_out", false, Backend::CUDA, ScalarType::Float);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, Backend::CUDA, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addr_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_out", false, Backend::CUDA, ScalarType::Int);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, Backend::CUDA, ScalarType::Int);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addr_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_out", false, Backend::CUDA, ScalarType::Long);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, Backend::CUDA, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addr_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_out", false, Backend::CUDA, ScalarType::Short);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, Backend::CUDA, ScalarType::Short);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addr_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_out", false, Backend::CUDA, ScalarType::Half);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, Backend::CUDA, ScalarType::Half);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addr_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_addr(const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {vec1.size(0),vec2.size(0)}, "_th_addr");
    return s__th_addr(b_self, vec1, vec2, beta, alpha);
}
Tensor s__th_addr(const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr", false, Backend::CUDA, ScalarType::Byte);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, Backend::CUDA, ScalarType::Byte);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr", false, Backend::CUDA, ScalarType::Char);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, Backend::CUDA, ScalarType::Char);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr", false, Backend::CUDA, ScalarType::Double);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, Backend::CUDA, ScalarType::Double);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr", false, Backend::CUDA, ScalarType::Float);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, Backend::CUDA, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr", false, Backend::CUDA, ScalarType::Int);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, Backend::CUDA, ScalarType::Int);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr", false, Backend::CUDA, ScalarType::Long);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, Backend::CUDA, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr", false, Backend::CUDA, ScalarType::Short);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, Backend::CUDA, ScalarType::Short);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr", false, Backend::CUDA, ScalarType::Half);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, Backend::CUDA, ScalarType::Half);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_addr(globalContext().getTHCState(), result_, self_, vec1_, vec2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addr not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addr_(Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_", false, Backend::CUDA, ScalarType::Byte);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, Backend::CUDA, ScalarType::Byte);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_addr(globalContext().getTHCState(), self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_", false, Backend::CUDA, ScalarType::Char);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, Backend::CUDA, ScalarType::Char);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_addr(globalContext().getTHCState(), self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_", false, Backend::CUDA, ScalarType::Double);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, Backend::CUDA, ScalarType::Double);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_addr(globalContext().getTHCState(), self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_", false, Backend::CUDA, ScalarType::Float);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, Backend::CUDA, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_addr(globalContext().getTHCState(), self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_", false, Backend::CUDA, ScalarType::Int);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, Backend::CUDA, ScalarType::Int);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_addr(globalContext().getTHCState(), self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_", false, Backend::CUDA, ScalarType::Long);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, Backend::CUDA, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_addr(globalContext().getTHCState(), self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_", false, Backend::CUDA, ScalarType::Short);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, Backend::CUDA, ScalarType::Short);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_addr(globalContext().getTHCState(), self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addr_", false, Backend::CUDA, ScalarType::Half);
            auto vec1_ = checked_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, Backend::CUDA, ScalarType::Half);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_addr(globalContext().getTHCState(), self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addr_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ger_out(Tensor & result, const Tensor & self, const Tensor & vec2) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ger_out", false, Backend::CUDA, ScalarType::Byte);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger_out", false, Backend::CUDA, ScalarType::Byte);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, uint8_t(0), uint8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ger_out", false, Backend::CUDA, ScalarType::Char);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger_out", false, Backend::CUDA, ScalarType::Char);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, int8_t(0), int8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ger_out", false, Backend::CUDA, ScalarType::Double);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger_out", false, Backend::CUDA, ScalarType::Double);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, double(0), double(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ger_out", false, Backend::CUDA, ScalarType::Float);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger_out", false, Backend::CUDA, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, float(0), float(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ger_out", false, Backend::CUDA, ScalarType::Int);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger_out", false, Backend::CUDA, ScalarType::Int);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, int(0), int(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ger_out", false, Backend::CUDA, ScalarType::Long);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger_out", false, Backend::CUDA, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, int64_t(0), int64_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ger_out", false, Backend::CUDA, ScalarType::Short);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger_out", false, Backend::CUDA, ScalarType::Short);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, int16_t(0), int16_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_ger_out", false, Backend::CUDA, ScalarType::Half);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger_out", false, Backend::CUDA, ScalarType::Half);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, Half(0), Half(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ger_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_ger(const Tensor & self, const Tensor & vec2) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger", false, Backend::CUDA, ScalarType::Byte);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, uint8_t(0), uint8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger", false, Backend::CUDA, ScalarType::Char);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, int8_t(0), int8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger", false, Backend::CUDA, ScalarType::Double);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, double(0), double(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger", false, Backend::CUDA, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, float(0), float(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger", false, Backend::CUDA, ScalarType::Int);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, int(0), int(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger", false, Backend::CUDA, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, int64_t(0), int64_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger", false, Backend::CUDA, ScalarType::Short);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, int16_t(0), int16_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_ger", false, Backend::CUDA, ScalarType::Half);
            auto vec2_ = checked_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addr(globalContext().getTHCState(), result_, result_, self_, vec2_, Half(0), Half(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ger not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_mv_out(Tensor & result, const Tensor & self, const Tensor & vec) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mv_out", false, Backend::CUDA, ScalarType::Byte);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv_out", false, Backend::CUDA, ScalarType::Byte);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, uint8_t(0), uint8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mv_out", false, Backend::CUDA, ScalarType::Char);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv_out", false, Backend::CUDA, ScalarType::Char);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, int8_t(0), int8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mv_out", false, Backend::CUDA, ScalarType::Double);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv_out", false, Backend::CUDA, ScalarType::Double);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, double(0), double(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mv_out", false, Backend::CUDA, ScalarType::Float);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv_out", false, Backend::CUDA, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, float(0), float(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mv_out", false, Backend::CUDA, ScalarType::Int);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv_out", false, Backend::CUDA, ScalarType::Int);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, int(0), int(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mv_out", false, Backend::CUDA, ScalarType::Long);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv_out", false, Backend::CUDA, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, int64_t(0), int64_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mv_out", false, Backend::CUDA, ScalarType::Short);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv_out", false, Backend::CUDA, ScalarType::Short);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, int16_t(0), int16_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mv_out", false, Backend::CUDA, ScalarType::Half);
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv_out", false, Backend::CUDA, ScalarType::Half);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, Half(0), Half(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mv_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_mv(const Tensor & self, const Tensor & vec) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv", false, Backend::CUDA, ScalarType::Byte);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, uint8_t(0), uint8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv", false, Backend::CUDA, ScalarType::Char);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, int8_t(0), int8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv", false, Backend::CUDA, ScalarType::Double);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, double(0), double(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv", false, Backend::CUDA, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, float(0), float(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv", false, Backend::CUDA, ScalarType::Int);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, int(0), int(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv", false, Backend::CUDA, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, int64_t(0), int64_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv", false, Backend::CUDA, ScalarType::Short);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, int16_t(0), int16_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mv", false, Backend::CUDA, ScalarType::Half);
            auto vec_ = checked_tensor_unwrap(vec, "vec", 2, "_th_mv", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmv(globalContext().getTHCState(), result_, result_, self_, vec_, Half(0), Half(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mv not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_mm_out(Tensor & result, const Tensor & self, const Tensor & mat2) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mm_out", false, Backend::CUDA, ScalarType::Byte);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm_out", false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, uint8_t(0), uint8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mm_out", false, Backend::CUDA, ScalarType::Char);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm_out", false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int8_t(0), int8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mm_out", false, Backend::CUDA, ScalarType::Double);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm_out", false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, double(0), double(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mm_out", false, Backend::CUDA, ScalarType::Float);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm_out", false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, float(0), float(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mm_out", false, Backend::CUDA, ScalarType::Int);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm_out", false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int(0), int(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mm_out", false, Backend::CUDA, ScalarType::Long);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm_out", false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int64_t(0), int64_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mm_out", false, Backend::CUDA, ScalarType::Short);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm_out", false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int16_t(0), int16_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_mm_out", false, Backend::CUDA, ScalarType::Half);
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm_out", false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, Half(0), Half(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_mm(const Tensor & self, const Tensor & mat2) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm", false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, uint8_t(0), uint8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm", false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int8_t(0), int8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm", false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, double(0), double(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm", false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, float(0), float(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm", false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int(0), int(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm", false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int64_t(0), int64_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm", false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int16_t(0), int16_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_mm", false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_addmm(globalContext().getTHCState(), result_, result_, self_, mat2_, Half(0), Half(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_bmm_out(Tensor & result, const Tensor & self, const Tensor & mat2) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_bmm_out", false, Backend::CUDA, ScalarType::Byte);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm_out", false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm_out", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, uint8_t(0), uint8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_bmm_out", false, Backend::CUDA, ScalarType::Char);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm_out", false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm_out", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int8_t(0), int8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_bmm_out", false, Backend::CUDA, ScalarType::Double);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm_out", false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, double(0), double(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_bmm_out", false, Backend::CUDA, ScalarType::Float);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm_out", false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, float(0), float(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_bmm_out", false, Backend::CUDA, ScalarType::Int);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm_out", false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm_out", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int(0), int(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_bmm_out", false, Backend::CUDA, ScalarType::Long);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm_out", false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm_out", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int64_t(0), int64_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_bmm_out", false, Backend::CUDA, ScalarType::Short);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm_out", false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm_out", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int16_t(0), int16_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_bmm_out", false, Backend::CUDA, ScalarType::Half);
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm_out", false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, Half(0), Half(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_bmm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_bmm(const Tensor & self, const Tensor & mat2) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm", false, Backend::CUDA, ScalarType::Byte);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, uint8_t(0), uint8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm", false, Backend::CUDA, ScalarType::Char);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int8_t(0), int8_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm", false, Backend::CUDA, ScalarType::Double);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, double(0), double(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm", false, Backend::CUDA, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, float(0), float(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm", false, Backend::CUDA, ScalarType::Int);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int(0), int(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm", false, Backend::CUDA, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int64_t(0), int64_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm", false, Backend::CUDA, ScalarType::Short);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, int16_t(0), int16_t(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),self.size(1),mat2.size(2) });
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_bmm", false, Backend::CUDA, ScalarType::Half);
            auto mat2_ = checked_tensor_unwrap(mat2, "mat2", 2, "_th_bmm", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_baddbmm(globalContext().getTHCState(), result_, result_, self_, mat2_, Half(0), Half(1));
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_bmm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {batch1.size(1),batch2.size(2)}, "_th_addbmm_out");
    return s__th_addbmm_out(result, b_self, batch1, batch2, beta, alpha);
}
Tensor & s__th_addbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Byte);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Byte);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Char);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Char);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Double);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Double);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Float);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Float);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Int);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Int);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Long);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Long);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Short);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Short);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Half);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Half);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addbmm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_addbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {batch1.size(1),batch2.size(2)}, "_th_addbmm");
    return s__th_addbmm(b_self, batch1, batch2, beta, alpha);
}
Tensor s__th_addbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm", false, Backend::CUDA, ScalarType::Byte);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, Backend::CUDA, ScalarType::Byte);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm", false, Backend::CUDA, ScalarType::Char);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, Backend::CUDA, ScalarType::Char);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm", false, Backend::CUDA, ScalarType::Double);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, Backend::CUDA, ScalarType::Double);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm", false, Backend::CUDA, ScalarType::Float);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, Backend::CUDA, ScalarType::Float);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm", false, Backend::CUDA, ScalarType::Int);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, Backend::CUDA, ScalarType::Int);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm", false, Backend::CUDA, ScalarType::Long);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, Backend::CUDA, ScalarType::Long);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm", false, Backend::CUDA, ScalarType::Short);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, Backend::CUDA, ScalarType::Short);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm", false, Backend::CUDA, ScalarType::Half);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, Backend::CUDA, ScalarType::Half);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_addbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addbmm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addbmm_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, Backend::CUDA, ScalarType::Byte);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, Backend::CUDA, ScalarType::Byte);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_addbmm(globalContext().getTHCState(), self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, Backend::CUDA, ScalarType::Char);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, Backend::CUDA, ScalarType::Char);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_addbmm(globalContext().getTHCState(), self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, Backend::CUDA, ScalarType::Double);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, Backend::CUDA, ScalarType::Double);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_addbmm(globalContext().getTHCState(), self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, Backend::CUDA, ScalarType::Float);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, Backend::CUDA, ScalarType::Float);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_addbmm(globalContext().getTHCState(), self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, Backend::CUDA, ScalarType::Int);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, Backend::CUDA, ScalarType::Int);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_addbmm(globalContext().getTHCState(), self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, Backend::CUDA, ScalarType::Long);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, Backend::CUDA, ScalarType::Long);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_addbmm(globalContext().getTHCState(), self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, Backend::CUDA, ScalarType::Short);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, Backend::CUDA, ScalarType::Short);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_addbmm(globalContext().getTHCState(), self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, Backend::CUDA, ScalarType::Half);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, Backend::CUDA, ScalarType::Half);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_addbmm(globalContext().getTHCState(), self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addbmm_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_baddbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {batch1.size(0),batch1.size(1),batch2.size(2)}, "_th_baddbmm_out");
    return s__th_baddbmm_out(result, b_self, batch1, batch2, beta, alpha);
}
Tensor & s__th_baddbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Byte);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Byte);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Char);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Char);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Char);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Double);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Double);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Float);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Float);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Int);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Int);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Long);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Long);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Short);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Short);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Short);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Half);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Half);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm_out", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_baddbmm_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_baddbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {batch1.size(0),batch1.size(1),batch2.size(2)}, "_th_baddbmm");
    return s__th_baddbmm(b_self, batch1, batch2, beta, alpha);
}
Tensor s__th_baddbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm", false, Backend::CUDA, ScalarType::Byte);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm", false, Backend::CUDA, ScalarType::Byte);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm", false, Backend::CUDA, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THCudaByteTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm", false, Backend::CUDA, ScalarType::Char);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm", false, Backend::CUDA, ScalarType::Char);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm", false, Backend::CUDA, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCudaCharTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm", false, Backend::CUDA, ScalarType::Double);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm", false, Backend::CUDA, ScalarType::Double);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THCudaDoubleTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm", false, Backend::CUDA, ScalarType::Float);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm", false, Backend::CUDA, ScalarType::Float);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THCudaTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm", false, Backend::CUDA, ScalarType::Int);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm", false, Backend::CUDA, ScalarType::Int);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm", false, Backend::CUDA, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THCudaIntTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm", false, Backend::CUDA, ScalarType::Long);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm", false, Backend::CUDA, ScalarType::Long);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm", false, Backend::CUDA, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THCudaLongTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm", false, Backend::CUDA, ScalarType::Short);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm", false, Backend::CUDA, ScalarType::Short);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm", false, Backend::CUDA, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THCudaShortTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_baddbmm", false, Backend::CUDA, ScalarType::Half);
            auto batch1_ = checked_tensor_unwrap(batch1, "batch1", 2, "_th_baddbmm", false, Backend::CUDA, ScalarType::Half);
            auto batch2_ = checked_tensor_unwrap(batch2, "batch2", 3, "_th_baddbmm", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toHalf();
            auto alpha_ = alpha.toHalf();
            THCudaHalfTensor_baddbmm(globalContext().getTHCState(), result_, self_, batch1_, batch2_, beta_, alpha_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_baddbmm not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_gels_out(Tensor & res1, Tensor & res2, const Tensor & self, const Tensor & A) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = checked_tensor_unwrap(res1, "res1", 0, "_th_gels_out", false, Backend::CUDA, ScalarType::Double);
            auto res2_ = checked_tensor_unwrap(res2, "res2", 0, "_th_gels_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gels_out", false, Backend::CUDA, ScalarType::Double);
            auto A_ = checked_tensor_unwrap(A, "A", 2, "_th_gels_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gels(globalContext().getTHCState(), res1_, res2_, self_, A_);
            bool maybe_scalar = self_->dim() == 0 && A_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1, "res1", 0, "_th_gels_out", false, Backend::CUDA, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2, "res2", 0, "_th_gels_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gels_out", false, Backend::CUDA, ScalarType::Float);
            auto A_ = checked_tensor_unwrap(A, "A", 2, "_th_gels_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gels(globalContext().getTHCState(), res1_, res2_, self_, A_);
            bool maybe_scalar = self_->dim() == 0 && A_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_gels_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_gels(const Tensor & self, const Tensor & A) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gels", false, Backend::CUDA, ScalarType::Double);
            auto A_ = checked_tensor_unwrap(A, "A", 2, "_th_gels", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_gels(globalContext().getTHCState(), res1_, res2_, self_, A_);
            bool maybe_scalar = self_->dim() == 0 && A_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_gels", false, Backend::CUDA, ScalarType::Float);
            auto A_ = checked_tensor_unwrap(A, "A", 2, "_th_gels", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_gels(globalContext().getTHCState(), res1_, res2_, self_, A_);
            bool maybe_scalar = self_->dim() == 0 && A_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_gels not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_eig_out(Tensor & res1, Tensor & res2, const Tensor & self, bool eigenvectors) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = checked_tensor_unwrap(res1, "res1", 0, "_th_eig_out", false, Backend::CUDA, ScalarType::Double);
            auto res2_ = checked_tensor_unwrap(res2, "res2", 0, "_th_eig_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eig_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geev(globalContext().getTHCState(), res1_, res2_, self_, eigenvectors);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1, "res1", 0, "_th_eig_out", false, Backend::CUDA, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2, "res2", 0, "_th_eig_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eig_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geev(globalContext().getTHCState(), res1_, res2_, self_, eigenvectors);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_eig_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_eig(const Tensor & self, bool eigenvectors) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eig", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geev(globalContext().getTHCState(), res1_, res2_, self_, eigenvectors);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_eig", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geev(globalContext().getTHCState(), res1_, res2_, self_, eigenvectors);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_eig not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_potri_out(Tensor & output, const Tensor & self, bool upper) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = checked_tensor_unwrap(output, "output", 0, "_th_potri_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_potri_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_potri(globalContext().getTHCState(), output_, self_, upper);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = checked_tensor_unwrap(output, "output", 0, "_th_potri_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_potri_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_potri(globalContext().getTHCState(), output_, self_, upper);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_potri_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_potri(const Tensor & self, bool upper) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_potri", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_potri(globalContext().getTHCState(), output_, self_, upper);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_potri", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_potri(globalContext().getTHCState(), output_, self_, upper);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_potri not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_geqrf_out(Tensor & res1, Tensor & res2, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = checked_tensor_unwrap(res1, "res1", 0, "_th_geqrf_out", false, Backend::CUDA, ScalarType::Double);
            auto res2_ = checked_tensor_unwrap(res2, "res2", 0, "_th_geqrf_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_geqrf_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geqrf(globalContext().getTHCState(), res1_, res2_, self_);
            bool maybe_scalar = false;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1, "res1", 0, "_th_geqrf_out", false, Backend::CUDA, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2, "res2", 0, "_th_geqrf_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_geqrf_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geqrf(globalContext().getTHCState(), res1_, res2_, self_);
            bool maybe_scalar = false;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_geqrf_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_geqrf(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_geqrf", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_geqrf(globalContext().getTHCState(), res1_, res2_, self_);
            bool maybe_scalar = false;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_geqrf", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_geqrf(globalContext().getTHCState(), res1_, res2_, self_);
            bool maybe_scalar = false;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_geqrf not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_multinomial_alias_setup_out(Tensor & J, Tensor & q, const Tensor & probs) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(J);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto probs_ = checked_tensor_unwrap(probs, "probs", 1, "_th_multinomial_alias_setup_out", false, Backend::CUDA, ScalarType::Double);
            auto J_ = checked_tensor_unwrap(J, "J", 1, "_th_multinomial_alias_setup_out", false, Backend::CUDA, ScalarType::Long);
            auto q_ = checked_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_setup_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_multinomialAliasSetup(globalContext().getTHCState(), probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(J, q);
            break;
        }
        case ScalarType::Float: {
            auto probs_ = checked_tensor_unwrap(probs, "probs", 1, "_th_multinomial_alias_setup_out", false, Backend::CUDA, ScalarType::Float);
            auto J_ = checked_tensor_unwrap(J, "J", 1, "_th_multinomial_alias_setup_out", false, Backend::CUDA, ScalarType::Long);
            auto q_ = checked_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_setup_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_multinomialAliasSetup(globalContext().getTHCState(), probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(J, q);
            break;
        }
        case ScalarType::Half: {
            auto probs_ = checked_tensor_unwrap(probs, "probs", 1, "_th_multinomial_alias_setup_out", false, Backend::CUDA, ScalarType::Half);
            auto J_ = checked_tensor_unwrap(J, "J", 1, "_th_multinomial_alias_setup_out", false, Backend::CUDA, ScalarType::Long);
            auto q_ = checked_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_setup_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_multinomialAliasSetup(globalContext().getTHCState(), probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(J, q);
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_setup_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_multinomial_alias_setup(const Tensor & probs) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(probs);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto probs_ = checked_tensor_unwrap(probs, "probs", 1, "_th_multinomial_alias_setup", false, Backend::CUDA, ScalarType::Double);
            auto J_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto J = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(J_));
            auto q_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto q = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(q_));
            THCudaDoubleTensor_multinomialAliasSetup(globalContext().getTHCState(), probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(J, q);
            break;
        }
        case ScalarType::Float: {
            auto probs_ = checked_tensor_unwrap(probs, "probs", 1, "_th_multinomial_alias_setup", false, Backend::CUDA, ScalarType::Float);
            auto J_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto J = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(J_));
            auto q_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto q = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(q_));
            THCudaTensor_multinomialAliasSetup(globalContext().getTHCState(), probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(J, q);
            break;
        }
        case ScalarType::Half: {
            auto probs_ = checked_tensor_unwrap(probs, "probs", 1, "_th_multinomial_alias_setup", false, Backend::CUDA, ScalarType::Half);
            auto J_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto J = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(J_));
            auto q_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto q = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(q_));
            THCudaHalfTensor_multinomialAliasSetup(globalContext().getTHCState(), probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(J, q);
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_setup not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_multinomial_alias_draw_out(Tensor & result, const Tensor & q, const Tensor & J, int64_t num_samples, Generator * generator) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(result);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_multinomial_alias_draw_out", false, Backend::CUDA, ScalarType::Long);
            auto q_ = checked_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_draw_out", false, Backend::CUDA, ScalarType::Double);
            auto J_ = checked_tensor_unwrap(J, "J", 2, "_th_multinomial_alias_draw_out", false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_multinomialAliasDraw(globalContext().getTHCState(), result_, q_, J_, num_samples, generator);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_multinomial_alias_draw_out", false, Backend::CUDA, ScalarType::Long);
            auto q_ = checked_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_draw_out", false, Backend::CUDA, ScalarType::Float);
            auto J_ = checked_tensor_unwrap(J, "J", 2, "_th_multinomial_alias_draw_out", false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_multinomialAliasDraw(globalContext().getTHCState(), result_, q_, J_, num_samples, generator);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_multinomial_alias_draw_out", false, Backend::CUDA, ScalarType::Long);
            auto q_ = checked_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_draw_out", false, Backend::CUDA, ScalarType::Half);
            auto J_ = checked_tensor_unwrap(J, "J", 2, "_th_multinomial_alias_draw_out", false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_multinomialAliasDraw(globalContext().getTHCState(), result_, q_, J_, num_samples, generator);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_draw_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_multinomial_alias_draw(const Tensor & q, const Tensor & J, int64_t num_samples, Generator * generator) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(q);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto q_ = checked_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_draw", false, Backend::CUDA, ScalarType::Double);
            auto J_ = checked_tensor_unwrap(J, "J", 2, "_th_multinomial_alias_draw", false, Backend::CUDA, ScalarType::Long);
            THCudaDoubleTensor_multinomialAliasDraw(globalContext().getTHCState(), result_, q_, J_, num_samples, generator);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto q_ = checked_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_draw", false, Backend::CUDA, ScalarType::Float);
            auto J_ = checked_tensor_unwrap(J, "J", 2, "_th_multinomial_alias_draw", false, Backend::CUDA, ScalarType::Long);
            THCudaTensor_multinomialAliasDraw(globalContext().getTHCState(), result_, q_, J_, num_samples, generator);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto q_ = checked_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_draw", false, Backend::CUDA, ScalarType::Half);
            auto J_ = checked_tensor_unwrap(J, "J", 2, "_th_multinomial_alias_draw", false, Backend::CUDA, ScalarType::Long);
            THCudaHalfTensor_multinomialAliasDraw(globalContext().getTHCState(), result_, q_, J_, num_samples, generator);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_draw not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_multinomial_out(Tensor & result, const Tensor & self, int64_t num_samples, bool replacement, Generator * generator) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_multinomial_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_multinomial_out", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement, generator);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_multinomial_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_multinomial_out", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement, generator);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_tensor_unwrap(result, "result", 0, "_th_multinomial_out", false, Backend::CUDA, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_multinomial_out", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement, generator);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_multinomial(const Tensor & self, int64_t num_samples, bool replacement, Generator * generator) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_multinomial", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement, generator);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_multinomial", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement, generator);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_multinomial", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement, generator);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_copy_ignoring_overlaps_(Tensor & self, const Tensor & src) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Byte);
            auto src_ = checked_tensor_unwrap(src, "src", 2, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Char);
            auto src_ = checked_tensor_unwrap(src, "src", 2, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Double);
            auto src_ = checked_tensor_unwrap(src, "src", 2, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Float);
            auto src_ = checked_tensor_unwrap(src, "src", 2, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Float);
            THCudaTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Int);
            auto src_ = checked_tensor_unwrap(src, "src", 2, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src, "src", 2, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Short);
            auto src_ = checked_tensor_unwrap(src, "src", 2, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Half);
            auto src_ = checked_tensor_unwrap(src, "src", 2, "_th_copy_ignoring_overlaps_", false, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_copy_ignoring_overlaps_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _th_cat_out(Tensor & self, TensorList tensors, int64_t dim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_tensor_unwrap(self, "self", 0, "_th_cat_out", false, Backend::CUDA, ScalarType::Bool);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_tensor_unwrap(self, "self", 0, "_th_cat_out", false, Backend::CUDA, ScalarType::Byte);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_tensor_unwrap(self, "self", 0, "_th_cat_out", false, Backend::CUDA, ScalarType::Char);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 0, "_th_cat_out", false, Backend::CUDA, ScalarType::Double);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 0, "_th_cat_out", false, Backend::CUDA, ScalarType::Float);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Float);
            THCudaTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_tensor_unwrap(self, "self", 0, "_th_cat_out", false, Backend::CUDA, ScalarType::Int);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self, "self", 0, "_th_cat_out", false, Backend::CUDA, ScalarType::Long);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_tensor_unwrap(self, "self", 0, "_th_cat_out", false, Backend::CUDA, ScalarType::Short);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 0, "_th_cat_out", false, Backend::CUDA, ScalarType::Half);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_cat_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _th_cat(TensorList tensors, int64_t dim) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(tensors);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Bool);
            THCudaBoolTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Byte);
            THCudaByteTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Char);
            THCudaCharTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Double);
            THCudaDoubleTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Float);
            THCudaTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Int);
            THCudaIntTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Long);
            THCudaLongTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Short);
            THCudaShortTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Half);
            THCudaHalfTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_cat not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_binary_cross_entropy_forward_out(Tensor & output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_binary_cross_entropy_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_binary_cross_entropy_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_binary_cross_entropy_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_binary_cross_entropy_forward_out", true, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 4, "_thnn_binary_cross_entropy_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_binary_cross_entropy_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_binary_cross_entropy_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_binary_cross_entropy_forward_out", true, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 4, "_thnn_binary_cross_entropy_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_binary_cross_entropy_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_binary_cross_entropy_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_binary_cross_entropy_forward_out", true, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 4, "_thnn_binary_cross_entropy_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_binary_cross_entropy_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_binary_cross_entropy_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_binary_cross_entropy_forward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_binary_cross_entropy_forward", false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_binary_cross_entropy_forward", true, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_binary_cross_entropy_forward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_binary_cross_entropy_forward", false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_binary_cross_entropy_forward", true, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_binary_cross_entropy_forward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_binary_cross_entropy_forward", false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_binary_cross_entropy_forward", true, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_binary_cross_entropy_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_binary_cross_entropy_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_binary_cross_entropy_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_binary_cross_entropy_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_binary_cross_entropy_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_binary_cross_entropy_backward_out", true, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_binary_cross_entropy_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_binary_cross_entropy_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_binary_cross_entropy_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_binary_cross_entropy_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_binary_cross_entropy_backward_out", true, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_binary_cross_entropy_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_binary_cross_entropy_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_binary_cross_entropy_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_binary_cross_entropy_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_binary_cross_entropy_backward_out", true, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_binary_cross_entropy_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_binary_cross_entropy_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_binary_cross_entropy_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_binary_cross_entropy_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_binary_cross_entropy_backward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_binary_cross_entropy_backward", false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_binary_cross_entropy_backward", true, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_binary_cross_entropy_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_binary_cross_entropy_backward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_binary_cross_entropy_backward", false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_binary_cross_entropy_backward", true, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_binary_cross_entropy_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_binary_cross_entropy_backward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_binary_cross_entropy_backward", false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_binary_cross_entropy_backward", true, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_l1_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_l1_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_l1_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_l1_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_l1_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_l1_loss_forward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_l1_loss_forward", false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_l1_loss_forward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_l1_loss_forward", false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_l1_loss_forward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_l1_loss_forward", false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_l1_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_l1_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_l1_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_l1_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_l1_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_l1_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_l1_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_l1_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_l1_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_l1_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_l1_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_l1_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_l1_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_l1_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_l1_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_l1_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_mse_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_mse_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_mse_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_mse_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_mse_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_mse_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_mse_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_mse_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_mse_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_mse_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_mse_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_mse_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_mse_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_mse_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_mse_loss_forward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_mse_loss_forward", false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_mse_loss_forward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_mse_loss_forward", false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_mse_loss_forward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_mse_loss_forward", false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_mse_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_mse_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_mse_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_mse_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_mse_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_mse_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_mse_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_mse_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_mse_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_mse_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_mse_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_mse_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_mse_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_mse_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_mse_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_mse_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_mse_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_mse_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_mse_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_mse_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_mse_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_mse_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_mse_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_mse_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_mse_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_mse_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_mse_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_mse_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_multi_margin_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_multi_margin_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_multi_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_multi_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight, "weight", 5, "_thnn_multi_margin_loss_forward_out", true, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 6, "_thnn_multi_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_multi_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_multi_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight, "weight", 5, "_thnn_multi_margin_loss_forward_out", true, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 6, "_thnn_multi_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_multi_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_multi_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight, "weight", 5, "_thnn_multi_margin_loss_forward_out", true, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 6, "_thnn_multi_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_multi_margin_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_multi_margin_loss_forward(const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_multi_margin_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_multi_margin_loss_forward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_multi_margin_loss_forward", false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight, "weight", 5, "_thnn_multi_margin_loss_forward", true, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_multi_margin_loss_forward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_multi_margin_loss_forward", false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight, "weight", 5, "_thnn_multi_margin_loss_forward", true, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_multi_margin_loss_forward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_multi_margin_loss_forward", false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight, "weight", 5, "_thnn_multi_margin_loss_forward", true, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_multi_margin_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_multi_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_multi_margin_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_multi_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_multi_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_multi_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight, "weight", 6, "_thnn_multi_margin_loss_backward_out", true, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 7, "_thnn_multi_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_multi_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_multi_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_multi_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight, "weight", 6, "_thnn_multi_margin_loss_backward_out", true, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 7, "_thnn_multi_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_multi_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_multi_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_multi_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight, "weight", 6, "_thnn_multi_margin_loss_backward_out", true, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 7, "_thnn_multi_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_multi_margin_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_multi_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_multi_margin_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_multi_margin_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_multi_margin_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_multi_margin_loss_backward", false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight, "weight", 6, "_thnn_multi_margin_loss_backward", true, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_multi_margin_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_multi_margin_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_multi_margin_loss_backward", false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight, "weight", 6, "_thnn_multi_margin_loss_backward", true, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_multi_margin_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_multi_margin_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_multi_margin_loss_backward", false, Backend::CUDA, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight, "weight", 6, "_thnn_multi_margin_loss_backward", true, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_multi_margin_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _thnn_multilabel_margin_loss_forward_out(Tensor & output, Tensor & is_target, const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || is_target.has_names() || self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_multilabel_margin_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_multilabel_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_multilabel_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Long);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_multilabel_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto is_target_ = checked_tensor_unwrap(is_target, "is_target", 3, "_thnn_multilabel_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor &, Tensor &>(output, is_target);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_multilabel_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_multilabel_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Long);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_multilabel_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto is_target_ = checked_tensor_unwrap(is_target, "is_target", 3, "_thnn_multilabel_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor &, Tensor &>(output, is_target);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_multilabel_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_multilabel_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Long);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_multilabel_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto is_target_ = checked_tensor_unwrap(is_target, "is_target", 3, "_thnn_multilabel_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor &, Tensor &>(output, is_target);
            break;
        }
        default:
            AT_ERROR("_thnn_multilabel_margin_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _thnn_multilabel_margin_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_multilabel_margin_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_multilabel_margin_loss_forward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_multilabel_margin_loss_forward", false, Backend::CUDA, ScalarType::Long);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto is_target_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto is_target = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(is_target_));
            THNN_CudaDoubleMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor, Tensor>(output, is_target);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_multilabel_margin_loss_forward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_multilabel_margin_loss_forward", false, Backend::CUDA, ScalarType::Long);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto is_target_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto is_target = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(is_target_));
            THNN_CudaMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor, Tensor>(output, is_target);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_multilabel_margin_loss_forward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_multilabel_margin_loss_forward", false, Backend::CUDA, ScalarType::Long);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto is_target_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto is_target = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(is_target_));
            THNN_CudaHalfMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor, Tensor>(output, is_target);
            break;
        }
        default:
            AT_ERROR("_thnn_multilabel_margin_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_multilabel_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, const Tensor & is_target) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || target.has_names() || is_target.has_names()) {
        AT_ERROR(
            "_thnn_multilabel_margin_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target, "is_target", 5, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target, "is_target", 5, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target, "is_target", 5, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_multilabel_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_multilabel_margin_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_multilabel_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, const Tensor & is_target) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || target.has_names() || is_target.has_names()) {
        AT_ERROR(
            "_thnn_multilabel_margin_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_multilabel_margin_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_multilabel_margin_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_multilabel_margin_loss_backward", false, Backend::CUDA, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target, "is_target", 5, "_thnn_multilabel_margin_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_multilabel_margin_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_multilabel_margin_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_multilabel_margin_loss_backward", false, Backend::CUDA, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target, "is_target", 5, "_thnn_multilabel_margin_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_multilabel_margin_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_multilabel_margin_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_multilabel_margin_loss_backward", false, Backend::CUDA, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target, "is_target", 5, "_thnn_multilabel_margin_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_multilabel_margin_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _thnn_nll_loss_forward_out(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || total_weight.has_names() || self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_nll_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_nll_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_nll_loss_forward_out", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_nll_loss_forward_out", true, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_nll_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 5, "_thnn_nll_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_nll_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_nll_loss_forward_out", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_nll_loss_forward_out", true, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_nll_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 5, "_thnn_nll_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_nll_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_nll_loss_forward_out", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_nll_loss_forward_out", true, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_nll_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 5, "_thnn_nll_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _thnn_nll_loss_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_nll_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_nll_loss_forward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_nll_loss_forward", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_nll_loss_forward", true, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_CudaDoubleClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_nll_loss_forward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_nll_loss_forward", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_nll_loss_forward", true, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_CudaClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_nll_loss_forward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_nll_loss_forward", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_nll_loss_forward", true, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_CudaHalfClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_nll_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || target.has_names() || weight.has_names() || total_weight.has_names()) {
        AT_ERROR(
            "_thnn_nll_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_nll_loss_backward_out", true, Backend::CUDA, ScalarType::Double);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 7, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 7, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_nll_loss_backward_out", true, Backend::CUDA, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 7, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 7, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_nll_loss_backward_out", true, Backend::CUDA, ScalarType::Half);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 7, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 7, "_thnn_nll_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_nll_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || target.has_names() || weight.has_names() || total_weight.has_names()) {
        AT_ERROR(
            "_thnn_nll_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_nll_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_nll_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_nll_loss_backward", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_nll_loss_backward", true, Backend::CUDA, ScalarType::Double);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 7, "_thnn_nll_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_nll_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_nll_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_nll_loss_backward", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_nll_loss_backward", true, Backend::CUDA, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 7, "_thnn_nll_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_nll_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_nll_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_nll_loss_backward", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_nll_loss_backward", true, Backend::CUDA, ScalarType::Half);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 7, "_thnn_nll_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _thnn_nll_loss2d_forward_out(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || total_weight.has_names() || self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_nll_loss2d_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_nll_loss2d_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_nll_loss2d_forward_out", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_nll_loss2d_forward_out", true, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_nll_loss2d_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 5, "_thnn_nll_loss2d_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_nll_loss2d_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_nll_loss2d_forward_out", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_nll_loss2d_forward_out", true, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_nll_loss2d_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 5, "_thnn_nll_loss2d_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_nll_loss2d_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_nll_loss2d_forward_out", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_nll_loss2d_forward_out", true, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_nll_loss2d_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 5, "_thnn_nll_loss2d_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss2d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _thnn_nll_loss2d_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_nll_loss2d_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_nll_loss2d_forward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_nll_loss2d_forward", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_nll_loss2d_forward", true, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_CudaDoubleSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_nll_loss2d_forward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_nll_loss2d_forward", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_nll_loss2d_forward", true, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_CudaSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_nll_loss2d_forward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_nll_loss2d_forward", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_nll_loss2d_forward", true, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_CudaHalfSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss2d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_nll_loss2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || target.has_names() || weight.has_names() || total_weight.has_names()) {
        AT_ERROR(
            "_thnn_nll_loss2d_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_nll_loss2d_backward_out", true, Backend::CUDA, ScalarType::Double);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 7, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 7, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_nll_loss2d_backward_out", true, Backend::CUDA, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 7, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 7, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_nll_loss2d_backward_out", true, Backend::CUDA, ScalarType::Half);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 7, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 7, "_thnn_nll_loss2d_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_nll_loss2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || target.has_names() || weight.has_names() || total_weight.has_names()) {
        AT_ERROR(
            "_thnn_nll_loss2d_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_nll_loss2d_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_nll_loss2d_backward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_nll_loss2d_backward", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_nll_loss2d_backward", true, Backend::CUDA, ScalarType::Double);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 7, "_thnn_nll_loss2d_backward", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_nll_loss2d_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_nll_loss2d_backward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_nll_loss2d_backward", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_nll_loss2d_backward", true, Backend::CUDA, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 7, "_thnn_nll_loss2d_backward", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_nll_loss2d_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_nll_loss2d_backward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_nll_loss2d_backward", false, Backend::CUDA, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 4, "_thnn_nll_loss2d_backward", true, Backend::CUDA, ScalarType::Half);
            auto total_weight_ = checked_tensor_unwrap(total_weight, "total_weight", 7, "_thnn_nll_loss2d_backward", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_smooth_l1_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_smooth_l1_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_smooth_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_smooth_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_smooth_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_smooth_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_smooth_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_smooth_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_smooth_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_smooth_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_smooth_l1_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_smooth_l1_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_smooth_l1_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_smooth_l1_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_smooth_l1_loss_forward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_smooth_l1_loss_forward", false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_smooth_l1_loss_forward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_smooth_l1_loss_forward", false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_smooth_l1_loss_forward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_smooth_l1_loss_forward", false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_smooth_l1_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_smooth_l1_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_smooth_l1_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_smooth_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_smooth_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_smooth_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_smooth_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_smooth_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_smooth_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_smooth_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_smooth_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_smooth_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_smooth_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_smooth_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_smooth_l1_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_smooth_l1_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_smooth_l1_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_smooth_l1_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_smooth_l1_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_smooth_l1_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_smooth_l1_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_smooth_l1_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_smooth_l1_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_smooth_l1_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_smooth_l1_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_smooth_l1_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_smooth_l1_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_smooth_l1_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_soft_margin_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_soft_margin_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_soft_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_soft_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_soft_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_soft_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_soft_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_soft_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_soft_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_soft_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_soft_margin_loss_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_soft_margin_loss_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_soft_margin_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_soft_margin_loss_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_soft_margin_loss_forward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_soft_margin_loss_forward", false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_soft_margin_loss_forward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_soft_margin_loss_forward", false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_soft_margin_loss_forward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 2, "_thnn_soft_margin_loss_forward", false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_soft_margin_loss_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_soft_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_soft_margin_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_soft_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_soft_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_soft_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_soft_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_soft_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_soft_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_soft_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_soft_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_soft_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_soft_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_soft_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_soft_margin_loss_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_soft_margin_loss_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_soft_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || target.has_names()) {
        AT_ERROR(
            "_thnn_soft_margin_loss_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_soft_margin_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_soft_margin_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_soft_margin_loss_backward", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_soft_margin_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_soft_margin_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_soft_margin_loss_backward", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_soft_margin_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_soft_margin_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto target_ = checked_tensor_unwrap(target, "target", 3, "_thnn_soft_margin_loss_backward", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_soft_margin_loss_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_elu_forward_out(Tensor & output, const Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_elu_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_elu_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 4, "_thnn_elu_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_elu_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 4, "_thnn_elu_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_elu_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 4, "_thnn_elu_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_elu_forward(const Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_elu_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_elu_forward", false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_elu_forward", false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_elu_forward", false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_elu_backward_out(Tensor & grad_input, const Tensor & grad_output, Scalar alpha, Scalar scale, Scalar input_scale, const Tensor & output) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || output.has_names()) {
        AT_ERROR(
            "_thnn_elu_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_elu_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_elu_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_elu_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_elu_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_elu_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_elu_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_elu_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_elu_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_elu_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_elu_backward(const Tensor & grad_output, Scalar alpha, Scalar scale, Scalar input_scale, const Tensor & output) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || output.has_names()) {
        AT_ERROR(
            "_thnn_elu_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_elu_backward", false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_elu_backward", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_elu_backward", false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_elu_backward", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_elu_backward", false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_elu_backward", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_elu_forward_(Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_elu_forward_ is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_elu_forward_", false, Backend::CUDA, ScalarType::Double);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            THNN_CudaDoubleELU_updateOutput(globalContext().getTHCState(), self_, self_, alpha_, scale_, input_scale_, true);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_elu_forward_", false, Backend::CUDA, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            THNN_CudaELU_updateOutput(globalContext().getTHCState(), self_, self_, alpha_, scale_, input_scale_, true);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_elu_forward_", false, Backend::CUDA, ScalarType::Half);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            THNN_CudaHalfELU_updateOutput(globalContext().getTHCState(), self_, self_, alpha_, scale_, input_scale_, true);
            return self;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_forward_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_glu_forward_out(Tensor & output, const Tensor & self, int64_t dim) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_glu_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_glu_forward_out", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_glu_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_glu_forward_out", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_glu_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_glu_forward_out", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_glu_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_glu_forward(const Tensor & self, int64_t dim) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_glu_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_glu_forward", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_glu_forward", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_glu_forward", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_glu_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, int64_t dim) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_glu_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_glu_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_glu_backward_out", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_glu_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_glu_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_glu_backward_out", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_glu_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_glu_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_glu_backward_out", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_glu_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_glu_backward(const Tensor & grad_output, const Tensor & self, int64_t dim) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_glu_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_glu_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_glu_backward", false, Backend::CUDA, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_glu_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_glu_backward", false, Backend::CUDA, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_glu_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_glu_backward", false, Backend::CUDA, ScalarType::Half);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_hardtanh_forward_out(Tensor & output, const Tensor & self, Scalar min_val, Scalar max_val) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_hardtanh_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_hardtanh_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_hardtanh_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_hardtanh_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_hardtanh_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_hardtanh_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_hardtanh_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_hardtanh_forward(const Tensor & self, Scalar min_val, Scalar max_val) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_hardtanh_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_hardtanh_forward", false, Backend::CUDA, ScalarType::Double);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_hardtanh_forward", false, Backend::CUDA, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_hardtanh_forward", false, Backend::CUDA, ScalarType::Half);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_hardtanh_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar min_val, Scalar max_val) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_hardtanh_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_hardtanh_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_hardtanh_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_hardtanh_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_hardtanh_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_hardtanh_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_hardtanh_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_hardtanh_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_hardtanh_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 4, "_thnn_hardtanh_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_hardtanh_backward(const Tensor & grad_output, const Tensor & self, Scalar min_val, Scalar max_val) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_hardtanh_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_hardtanh_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_hardtanh_backward", false, Backend::CUDA, ScalarType::Double);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_hardtanh_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_hardtanh_backward", false, Backend::CUDA, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_hardtanh_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_hardtanh_backward", false, Backend::CUDA, ScalarType::Half);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_hardtanh_forward_(Tensor & self, Scalar min_val, Scalar max_val) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_hardtanh_forward_ is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_hardtanh_forward_", false, Backend::CUDA, ScalarType::Double);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            THNN_CudaDoubleHardTanh_updateOutput(globalContext().getTHCState(), self_, self_, min_val_, max_val_, true);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_hardtanh_forward_", false, Backend::CUDA, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            THNN_CudaHardTanh_updateOutput(globalContext().getTHCState(), self_, self_, min_val_, max_val_, true);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_hardtanh_forward_", false, Backend::CUDA, ScalarType::Half);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            THNN_CudaHalfHardTanh_updateOutput(globalContext().getTHCState(), self_, self_, min_val_, max_val_, true);
            return self;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_forward_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_leaky_relu_forward_out(Tensor & output, const Tensor & self, Scalar negative_slope) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_leaky_relu_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_leaky_relu_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_leaky_relu_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_leaky_relu_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_leaky_relu_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_leaky_relu_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_leaky_relu_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_leaky_relu_forward(const Tensor & self, Scalar negative_slope) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_leaky_relu_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_leaky_relu_forward", false, Backend::CUDA, ScalarType::Double);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_leaky_relu_forward", false, Backend::CUDA, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_leaky_relu_forward", false, Backend::CUDA, ScalarType::Half);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_leaky_relu_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar negative_slope) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_leaky_relu_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_leaky_relu_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_leaky_relu_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_leaky_relu_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_leaky_relu_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_leaky_relu_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_leaky_relu_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_leaky_relu_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_leaky_relu_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_leaky_relu_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_leaky_relu_backward(const Tensor & grad_output, const Tensor & self, Scalar negative_slope) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_leaky_relu_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_leaky_relu_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_leaky_relu_backward", false, Backend::CUDA, ScalarType::Double);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_leaky_relu_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_leaky_relu_backward", false, Backend::CUDA, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_leaky_relu_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_leaky_relu_backward", false, Backend::CUDA, ScalarType::Half);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_leaky_relu_forward_(Tensor & self, Scalar negative_slope) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_leaky_relu_forward_ is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_leaky_relu_forward_", false, Backend::CUDA, ScalarType::Double);
            auto negative_slope_ = negative_slope.toDouble();
            THNN_CudaDoubleLeakyReLU_updateOutput(globalContext().getTHCState(), self_, self_, negative_slope_, true);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_leaky_relu_forward_", false, Backend::CUDA, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            THNN_CudaLeakyReLU_updateOutput(globalContext().getTHCState(), self_, self_, negative_slope_, true);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_leaky_relu_forward_", false, Backend::CUDA, ScalarType::Half);
            auto negative_slope_ = negative_slope.toDouble();
            THNN_CudaHalfLeakyReLU_updateOutput(globalContext().getTHCState(), self_, self_, negative_slope_, true);
            return self;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_forward_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _thnn_log_sigmoid_forward_out(Tensor & output, Tensor & buffer, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || buffer.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_log_sigmoid_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_log_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 1, "_thnn_log_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto buffer_ = checked_tensor_unwrap(buffer, "buffer", 1, "_thnn_log_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, buffer);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_log_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 1, "_thnn_log_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto buffer_ = checked_tensor_unwrap(buffer, "buffer", 1, "_thnn_log_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, buffer);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_log_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 1, "_thnn_log_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto buffer_ = checked_tensor_unwrap(buffer, "buffer", 1, "_thnn_log_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, buffer);
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _thnn_log_sigmoid_forward(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_log_sigmoid_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_log_sigmoid_forward", false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto buffer_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto buffer = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(buffer_));
            THNN_CudaDoubleLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, buffer);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_log_sigmoid_forward", false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto buffer_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto buffer = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(buffer_));
            THNN_CudaLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, buffer);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_log_sigmoid_forward", false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto buffer_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto buffer = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(buffer_));
            THNN_CudaHalfLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, buffer);
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_log_sigmoid_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & buffer) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || buffer.has_names()) {
        AT_ERROR(
            "_thnn_log_sigmoid_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_log_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_log_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto buffer_ = checked_tensor_unwrap(buffer, "buffer", 3, "_thnn_log_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_log_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_log_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_log_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto buffer_ = checked_tensor_unwrap(buffer, "buffer", 3, "_thnn_log_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_log_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_log_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_log_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto buffer_ = checked_tensor_unwrap(buffer, "buffer", 3, "_thnn_log_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_log_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_log_sigmoid_backward(const Tensor & grad_output, const Tensor & self, const Tensor & buffer) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || buffer.has_names()) {
        AT_ERROR(
            "_thnn_log_sigmoid_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_log_sigmoid_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_log_sigmoid_backward", false, Backend::CUDA, ScalarType::Double);
            auto buffer_ = checked_tensor_unwrap(buffer, "buffer", 3, "_thnn_log_sigmoid_backward", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_log_sigmoid_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_log_sigmoid_backward", false, Backend::CUDA, ScalarType::Float);
            auto buffer_ = checked_tensor_unwrap(buffer, "buffer", 3, "_thnn_log_sigmoid_backward", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_log_sigmoid_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_log_sigmoid_backward", false, Backend::CUDA, ScalarType::Half);
            auto buffer_ = checked_tensor_unwrap(buffer, "buffer", 3, "_thnn_log_sigmoid_backward", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_rrelu_with_noise_forward_out(Tensor & output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names() || noise.has_names()) {
        AT_ERROR(
            "_thnn_rrelu_with_noise_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_rrelu_with_noise_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 2, "_thnn_rrelu_with_noise_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 6, "_thnn_rrelu_with_noise_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, generator);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_rrelu_with_noise_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 2, "_thnn_rrelu_with_noise_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 6, "_thnn_rrelu_with_noise_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, generator);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_rrelu_with_noise_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 2, "_thnn_rrelu_with_noise_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 6, "_thnn_rrelu_with_noise_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, generator);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_rrelu_with_noise_forward(const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || noise.has_names()) {
        AT_ERROR(
            "_thnn_rrelu_with_noise_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_rrelu_with_noise_forward", false, Backend::CUDA, ScalarType::Double);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 2, "_thnn_rrelu_with_noise_forward", false, Backend::CUDA, ScalarType::Double);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, generator);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_rrelu_with_noise_forward", false, Backend::CUDA, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 2, "_thnn_rrelu_with_noise_forward", false, Backend::CUDA, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, generator);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_rrelu_with_noise_forward", false, Backend::CUDA, ScalarType::Half);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 2, "_thnn_rrelu_with_noise_forward", false, Backend::CUDA, ScalarType::Half);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, generator);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_rrelu_with_noise_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || noise.has_names()) {
        AT_ERROR(
            "_thnn_rrelu_with_noise_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_rrelu_with_noise_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_rrelu_with_noise_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 3, "_thnn_rrelu_with_noise_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 6, "_thnn_rrelu_with_noise_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_rrelu_with_noise_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_rrelu_with_noise_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 3, "_thnn_rrelu_with_noise_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 6, "_thnn_rrelu_with_noise_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_rrelu_with_noise_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_rrelu_with_noise_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 3, "_thnn_rrelu_with_noise_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 6, "_thnn_rrelu_with_noise_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_rrelu_with_noise_backward(const Tensor & grad_output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || noise.has_names()) {
        AT_ERROR(
            "_thnn_rrelu_with_noise_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_rrelu_with_noise_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_rrelu_with_noise_backward", false, Backend::CUDA, ScalarType::Double);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 3, "_thnn_rrelu_with_noise_backward", false, Backend::CUDA, ScalarType::Double);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_rrelu_with_noise_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_rrelu_with_noise_backward", false, Backend::CUDA, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 3, "_thnn_rrelu_with_noise_backward", false, Backend::CUDA, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_rrelu_with_noise_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_rrelu_with_noise_backward", false, Backend::CUDA, ScalarType::Half);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 3, "_thnn_rrelu_with_noise_backward", false, Backend::CUDA, ScalarType::Half);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_rrelu_with_noise_forward_(Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || noise.has_names()) {
        AT_ERROR(
            "_thnn_rrelu_with_noise_forward_ is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_rrelu_with_noise_forward_", false, Backend::CUDA, ScalarType::Double);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 2, "_thnn_rrelu_with_noise_forward_", false, Backend::CUDA, ScalarType::Double);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            THNN_CudaDoubleRReLU_updateOutput(globalContext().getTHCState(), self_, self_, noise_, lower_, upper_, training, true, generator);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_rrelu_with_noise_forward_", false, Backend::CUDA, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 2, "_thnn_rrelu_with_noise_forward_", false, Backend::CUDA, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            THNN_CudaRReLU_updateOutput(globalContext().getTHCState(), self_, self_, noise_, lower_, upper_, training, true, generator);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_rrelu_with_noise_forward_", false, Backend::CUDA, ScalarType::Half);
            auto noise_ = checked_tensor_unwrap(noise, "noise", 2, "_thnn_rrelu_with_noise_forward_", false, Backend::CUDA, ScalarType::Half);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            THNN_CudaHalfRReLU_updateOutput(globalContext().getTHCState(), self_, self_, noise_, lower_, upper_, training, true, generator);
            return self;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_forward_ not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_softplus_forward_out(Tensor & output, const Tensor & self, Scalar beta, Scalar threshold) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_softplus_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_softplus_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_softplus_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_softplus_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_softplus_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_softplus_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 3, "_thnn_softplus_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_softplus_forward(const Tensor & self, Scalar beta, Scalar threshold) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_softplus_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_softplus_forward", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_softplus_forward", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_softplus_forward", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_softplus_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || output.has_names()) {
        AT_ERROR(
            "_thnn_softplus_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softplus_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_softplus_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_softplus_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_softplus_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softplus_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_softplus_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_softplus_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_softplus_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softplus_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_softplus_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_softplus_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_softplus_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_softplus_backward(const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || output.has_names()) {
        AT_ERROR(
            "_thnn_softplus_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softplus_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_softplus_backward", false, Backend::CUDA, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_softplus_backward", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softplus_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_softplus_backward", false, Backend::CUDA, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_softplus_backward", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softplus_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_softplus_backward", false, Backend::CUDA, ScalarType::Half);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 5, "_thnn_softplus_backward", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_softshrink_forward_out(Tensor & output, const Tensor & self, Scalar lambd) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_softshrink_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_softshrink_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto lambd_ = lambd.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_softshrink_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_softshrink_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto lambd_ = lambd.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_softshrink_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_softshrink_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto lambd_ = lambd.toDouble();
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_softshrink_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softshrink_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_softshrink_forward(const Tensor & self, Scalar lambd) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_softshrink_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_softshrink_forward", false, Backend::CUDA, ScalarType::Double);
            auto lambd_ = lambd.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_softshrink_forward", false, Backend::CUDA, ScalarType::Float);
            auto lambd_ = lambd.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_softshrink_forward", false, Backend::CUDA, ScalarType::Half);
            auto lambd_ = lambd.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softshrink_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_softshrink_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar lambd) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_softshrink_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softshrink_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_softshrink_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_softshrink_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softshrink_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_softshrink_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_softshrink_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softshrink_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_softshrink_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_softshrink_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softshrink_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_softshrink_backward(const Tensor & grad_output, const Tensor & self, Scalar lambd) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_softshrink_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softshrink_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_softshrink_backward", false, Backend::CUDA, ScalarType::Double);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softshrink_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_softshrink_backward", false, Backend::CUDA, ScalarType::Float);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softshrink_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_softshrink_backward", false, Backend::CUDA, ScalarType::Half);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softshrink_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_sigmoid_forward_out(Tensor & output, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_sigmoid_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 1, "_thnn_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 1, "_thnn_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 1, "_thnn_sigmoid_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_sigmoid_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_sigmoid_forward(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_sigmoid_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_sigmoid_forward", false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_sigmoid_forward", false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_sigmoid_forward", false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_sigmoid_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_sigmoid_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & output) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || output.has_names()) {
        AT_ERROR(
            "_thnn_sigmoid_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 2, "_thnn_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 2, "_thnn_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 2, "_thnn_sigmoid_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_sigmoid_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_sigmoid_backward(const Tensor & grad_output, const Tensor & output) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || output.has_names()) {
        AT_ERROR(
            "_thnn_sigmoid_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_sigmoid_backward", false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_sigmoid_backward", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_sigmoid_backward", false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_sigmoid_backward", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_sigmoid_backward", false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_sigmoid_backward", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_sigmoid_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_tanh_forward_out(Tensor & output, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_tanh_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_tanh_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 1, "_thnn_tanh_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_tanh_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 1, "_thnn_tanh_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_tanh_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 1, "_thnn_tanh_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_tanh_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_tanh_forward(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_tanh_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_tanh_forward", false, Backend::CUDA, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_tanh_forward", false, Backend::CUDA, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_tanh_forward", false, Backend::CUDA, ScalarType::Half);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_tanh_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_tanh_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & output) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_output.has_names() || output.has_names()) {
        AT_ERROR(
            "_thnn_tanh_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_tanh_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_tanh_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 2, "_thnn_tanh_backward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_tanh_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_tanh_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 2, "_thnn_tanh_backward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_tanh_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_tanh_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 2, "_thnn_tanh_backward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_tanh_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_tanh_backward(const Tensor & grad_output, const Tensor & output) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || output.has_names()) {
        AT_ERROR(
            "_thnn_tanh_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_tanh_backward", false, Backend::CUDA, ScalarType::Double);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_tanh_backward", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaDoubleTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_tanh_backward", false, Backend::CUDA, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_tanh_backward", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_tanh_backward", false, Backend::CUDA, ScalarType::Half);
            auto output_ = checked_tensor_unwrap(output, "output", 2, "_thnn_tanh_backward", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_CudaHalfTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_tanh_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> _thnn_conv2d_forward_out(Tensor & output, Tensor & finput, Tensor & fgrad_input, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || finput.has_names() || fgrad_input.has_names() || self.has_names() || weight.has_names() || bias.has_names()) {
        AT_ERROR(
            "_thnn_conv2d_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 2, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias, "bias", 4, "_thnn_conv2d_forward_out", true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = checked_tensor_unwrap(output, "output", 6, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto finput_ = checked_tensor_unwrap(finput, "finput", 6, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input, "fgrad_input", 6, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 2, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias, "bias", 4, "_thnn_conv2d_forward_out", true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = checked_tensor_unwrap(output, "output", 6, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto finput_ = checked_tensor_unwrap(finput, "finput", 6, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input, "fgrad_input", 6, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 2, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias, "bias", 4, "_thnn_conv2d_forward_out", true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = checked_tensor_unwrap(output, "output", 6, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto finput_ = checked_tensor_unwrap(finput, "finput", 6, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input, "fgrad_input", 6, "_thnn_conv2d_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
            break;
        }
        default:
            AT_ERROR("_thnn_conv2d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> _thnn_conv2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || weight.has_names() || bias.has_names()) {
        AT_ERROR(
            "_thnn_conv2d_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_conv2d_forward", false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 2, "_thnn_conv2d_forward", false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias, "bias", 4, "_thnn_conv2d_forward", true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
            auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
            THNN_CudaDoubleSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_conv2d_forward", false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 2, "_thnn_conv2d_forward", false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias, "bias", 4, "_thnn_conv2d_forward", true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
            auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
            THNN_CudaSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_conv2d_forward", false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 2, "_thnn_conv2d_forward", false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias, "bias", 4, "_thnn_conv2d_forward", true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
            auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
            THNN_CudaHalfSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
            break;
        }
        default:
            AT_ERROR("_thnn_conv2d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> _thnn_conv2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, const Tensor & fgrad_input) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_weight.has_names() || grad_bias.has_names() || grad_output.has_names() || self.has_names() || weight.has_names() || finput.has_names() || fgrad_input.has_names()) {
        AT_ERROR(
            "_thnn_conv2d_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput, "finput", 7, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input, "fgrad_input", 8, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 8, "_thnn_conv2d_backward_out", true, Backend::CUDA, ScalarType::Double);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight, "grad_weight", 8, "_thnn_conv2d_backward_out", true, Backend::CUDA, ScalarType::Double);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias, "grad_bias", 8, "_thnn_conv2d_backward_out", true, Backend::CUDA, ScalarType::Double);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaDoubleSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaDoubleSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput, "finput", 7, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input, "fgrad_input", 8, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 8, "_thnn_conv2d_backward_out", true, Backend::CUDA, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight, "grad_weight", 8, "_thnn_conv2d_backward_out", true, Backend::CUDA, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias, "grad_bias", 8, "_thnn_conv2d_backward_out", true, Backend::CUDA, ScalarType::Float);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput, "finput", 7, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input, "fgrad_input", 8, "_thnn_conv2d_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 8, "_thnn_conv2d_backward_out", true, Backend::CUDA, ScalarType::Half);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight, "grad_weight", 8, "_thnn_conv2d_backward_out", true, Backend::CUDA, ScalarType::Half);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias, "grad_bias", 8, "_thnn_conv2d_backward_out", true, Backend::CUDA, ScalarType::Half);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaHalfSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> _thnn_conv2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, const Tensor & fgrad_input, std::array<bool,3> output_mask) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || weight.has_names() || finput.has_names() || fgrad_input.has_names()) {
        AT_ERROR(
            "_thnn_conv2d_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput, "finput", 7, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Double);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input, "fgrad_input", 8, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Double);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaDoubleSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaDoubleSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput, "finput", 7, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input, "fgrad_input", 8, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Float);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput, "finput", 7, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Half);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input, "fgrad_input", 8, "_thnn_conv2d_backward", false, Backend::CUDA, ScalarType::Half);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_CudaHalfSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_conv_depthwise2d_forward_out(Tensor & output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) {
#ifdef BUILD_NAMEDTENSOR
    if (output.has_names() || self.has_names() || weight.has_names() || bias.has_names()) {
        AT_ERROR(
            "_thnn_conv_depthwise2d_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_conv_depthwise2d_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 2, "_thnn_conv_depthwise2d_forward_out", false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias, "bias", 4, "_thnn_conv_depthwise2d_forward_out", true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output, "output", 7, "_thnn_conv_depthwise2d_forward_out", false, Backend::CUDA, ScalarType::Double);
            THNN_CudaDoubleSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_conv_depthwise2d_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 2, "_thnn_conv_depthwise2d_forward_out", false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias, "bias", 4, "_thnn_conv_depthwise2d_forward_out", true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output, "output", 7, "_thnn_conv_depthwise2d_forward_out", false, Backend::CUDA, ScalarType::Float);
            THNN_CudaSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_conv_depthwise2d_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 2, "_thnn_conv_depthwise2d_forward_out", false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias, "bias", 4, "_thnn_conv_depthwise2d_forward_out", true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output, "output", 7, "_thnn_conv_depthwise2d_forward_out", false, Backend::CUDA, ScalarType::Half);
            THNN_CudaHalfSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_conv_depthwise2d_forward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_conv_depthwise2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || weight.has_names() || bias.has_names()) {
        AT_ERROR(
            "_thnn_conv_depthwise2d_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_conv_depthwise2d_forward", false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 2, "_thnn_conv_depthwise2d_forward", false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias, "bias", 4, "_thnn_conv_depthwise2d_forward", true, Backend::CUDA, ScalarType::Double);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaDoubleSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_conv_depthwise2d_forward", false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 2, "_thnn_conv_depthwise2d_forward", false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias, "bias", 4, "_thnn_conv_depthwise2d_forward", true, Backend::CUDA, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
            return output;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_tensor_unwrap(self, "self", 1, "_thnn_conv_depthwise2d_forward", false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 2, "_thnn_conv_depthwise2d_forward", false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias, "bias", 4, "_thnn_conv_depthwise2d_forward", true, Backend::CUDA, ScalarType::Half);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_CudaHalfSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_conv_depthwise2d_forward not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _thnn_conv_depthwise2d_backward_out(Tensor & grad_input, Tensor & grad_weight, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_input.has_names() || grad_weight.has_names() || grad_output.has_names() || self.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_conv_depthwise2d_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_conv_depthwise2d_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_conv_depthwise2d_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_conv_depthwise2d_backward_out", false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 7, "_thnn_conv_depthwise2d_backward_out", true, Backend::CUDA, ScalarType::Double);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight, "grad_weight", 7, "_thnn_conv_depthwise2d_backward_out", true, Backend::CUDA, ScalarType::Double);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            if (grad_input_) THNN_CudaDoubleSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_) THNN_CudaDoubleSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &>(grad_input, grad_weight);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_conv_depthwise2d_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_conv_depthwise2d_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_conv_depthwise2d_backward_out", false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 7, "_thnn_conv_depthwise2d_backward_out", true, Backend::CUDA, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight, "grad_weight", 7, "_thnn_conv_depthwise2d_backward_out", true, Backend::CUDA, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            if (grad_input_) THNN_CudaSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_) THNN_CudaSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &>(grad_input, grad_weight);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_conv_depthwise2d_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_conv_depthwise2d_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_conv_depthwise2d_backward_out", false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto grad_input_ = checked_tensor_unwrap(grad_input, "grad_input", 7, "_thnn_conv_depthwise2d_backward_out", true, Backend::CUDA, ScalarType::Half);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight, "grad_weight", 7, "_thnn_conv_depthwise2d_backward_out", true, Backend::CUDA, ScalarType::Half);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            if (grad_input_) THNN_CudaHalfSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_) THNN_CudaHalfSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &>(grad_input, grad_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_depthwise2d_backward_out not supported on CUDAType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _thnn_conv_depthwise2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, std::array<bool,2> output_mask) {
#ifdef BUILD_NAMEDTENSOR
    if (grad_output.has_names() || self.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_conv_depthwise2d_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_conv_depthwise2d_backward", false, Backend::CUDA, ScalarType::Double);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_conv_depthwise2d_backward", false, Backend::CUDA, ScalarType::Double);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_conv_depthwise2d_backward", false, Backend::CUDA, ScalarType::Double);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            if (grad_input_) THNN_CudaDoubleSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_) THNN_CudaDoubleSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor>(grad_input, grad_weight);
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_conv_depthwise2d_backward", false, Backend::CUDA, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_conv_depthwise2d_backward", false, Backend::CUDA, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_conv_depthwise2d_backward", false, Backend::CUDA, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            if (grad_input_) THNN_CudaSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_) THNN_CudaSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor>(grad_input, grad_weight);
            break;
        }
        case ScalarType::Half: {
            auto grad_output_ = checked_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_conv_depthwise2d_backward", false, Backend::CUDA, ScalarType::Half);
            auto self_ = checked_tensor_unwrap(self, "self", 2, "_thnn_conv_depthwise2d_backward", false, Backend::CUDA, ScalarType::Half);
            auto weight_ = checked_tensor_unwrap(weight, "weight", 3, "_thnn_conv_depthwise2d_backward", false, Backend::CUDA, ScalarType::Half);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CUDATensorId).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            if (grad_input_) THNN_CudaHalfSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_) THNN_CudaHalfSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor>(grad_input, grad_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_depthwise2d_backward not supported on CUDAType for ", dispatch_scalar_type);
    }
}

} // namespace th
} // namespace legacy
} // namespace native
} // namespace at
