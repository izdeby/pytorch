// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#define __STDC_FORMAT_MACROS

#include <ATen/SparseCUDAType.h>

// @generated by aten/src/ATen/gen.py

#include <ATen/CUDAGenerator.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/core/op_registration/op_registration.h>
#include <ATen/core/EnableNamedTensor.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/DeviceGuard.h>
#include <ATen/cuda/ATenCUDAGeneral.h>
#include <ATen/cuda/CUDADevice.h>
#include <ATen/cuda/CUDAContext.h>

namespace at {

Tensor SparseCUDAType::add(const Tensor & self, const Tensor & other, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::add_sparse(self, other, alpha);
}
Tensor & SparseCUDAType::add_(Tensor & self, const Tensor & other, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::add_sparse_(self, other, alpha);
}
Tensor & SparseCUDAType::add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::add_out_sparse_cuda(out, self, other, alpha);
}
Tensor SparseCUDAType::div(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::div_sparse(self, other);
}
Tensor & SparseCUDAType::div_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::div_sparse_(self, other);
}
Tensor & SparseCUDAType::div_out(Tensor & out, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::div_out_sparse_zerodim(out, self, other);
}
Tensor SparseCUDAType::empty(IntArrayRef size, const TensorOptions & options, c10::optional<MemoryFormat> memory_format) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const DeviceGuard device_guard(options.device());
    return at::native::empty_sparse(size, options, memory_format);
}
Tensor & SparseCUDAType::log1p_(Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log1p_sparse_(self);
}
Tensor & SparseCUDAType::log1p_out(Tensor & out, const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log1p_out_sparse(out, self);
}
Tensor SparseCUDAType::mm(const Tensor & self, const Tensor & mat2) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sparse_mm(self, mat2);
}
Tensor & SparseCUDAType::mm_out(Tensor & out, const Tensor & self, const Tensor & mat2) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sparse_mm_out(out, self, mat2);
}
Tensor SparseCUDAType::mul(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mul_sparse(self, other);
}
Tensor & SparseCUDAType::mul_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mul_sparse_(self, other);
}
Tensor & SparseCUDAType::mul_out(Tensor & out, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mul_out_sparse_cuda(out, self, other);
}
Tensor SparseCUDAType::narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "narrow_copy is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::narrow_copy_sparse(self, dim, start, length);
}
Tensor & SparseCUDAType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR
    if (out.has_names() || self.has_names() || mat1.has_names() || mat2.has_names()) {
        AT_ERROR(
            "sspaddmm_out is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sspaddmm_out_cuda(out, self, mat1, mat2, beta, alpha);
}
Tensor SparseCUDAType::native_norm(const Tensor & self, Scalar p) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "native_norm is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::norm_sparse(self, p);
}
Tensor SparseCUDAType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) {
#ifdef BUILD_NAMEDTENSOR
    if (grad.has_names() || self.has_names()) {
        AT_ERROR(
            "_sparse_sum_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sparse_sum_backward_cuda(grad, self, dim);
}
Tensor SparseCUDAType::clone(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::clone_sparse(self);
}
Tensor & SparseCUDAType::resize_as_(Tensor & self, const Tensor & the_template) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::resize_as_sparse_(self, the_template);
}
Tensor & SparseCUDAType::pow_out(Tensor & out, const Tensor & self, Scalar exponent) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pow_out_sparse_scalar(out, self, exponent);
}
Tensor SparseCUDAType::pow(const Tensor & self, Scalar exponent) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pow_sparse_scalar(self, exponent);
}
Tensor & SparseCUDAType::zero_(Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::zero_sparse_(self);
}
Tensor & SparseCUDAType::sub_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sub_out_sparse(out, self, other, alpha);
}
Tensor SparseCUDAType::sub(const Tensor & self, const Tensor & other, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sub_sparse(self, other, alpha);
}
Tensor & SparseCUDAType::sub_(Tensor & self, const Tensor & other, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sub_sparse_(self, other, alpha);
}
Tensor & SparseCUDAType::addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addmm_out_sparse_dense_cuda(out, self, mat1, mat2, beta, alpha);
}
Tensor SparseCUDAType::addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addmm_sparse_dense_cuda(self, mat1, mat2, beta, alpha);
}
Tensor & SparseCUDAType::addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::s_addmm_sparse_dense_cuda_(self, mat1, mat2, beta, alpha);
}
Tensor SparseCUDAType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const DeviceGuard device_guard(options.device());
    return at::native::new_with_dims_sparse(sparse_dim, dense_dim, size, options);
}
Tensor SparseCUDAType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) {
#ifdef BUILD_NAMEDTENSOR
    if (indices.has_names() || values.has_names()) {
        AT_ERROR(
            "_sparse_coo_tensor_with_dims_and_tensors is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const DeviceGuard device_guard(options.device());
    return at::native::new_with_dims_and_tensor_sparse(sparse_dim, dense_dim, size, indices, values, options);
}
Tensor & SparseCUDAType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "sparse_resize_ is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sparse_resize_(self, size, sparse_dim, dense_dim);
}
Tensor & SparseCUDAType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "sparse_resize_and_clear_ is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sparse_resize_and_clear_(self, size, sparse_dim, dense_dim);
}
Tensor SparseCUDAType::sparse_mask(const Tensor & self, const Tensor & mask) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || mask.has_names()) {
        AT_ERROR(
            "sparse_mask is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sparse_mask_cuda(self, mask);
}
Tensor SparseCUDAType::to_dense(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "to_dense is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sparse_to_dense(self);
}
int64_t SparseCUDAType::sparse_dim(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "sparse_dim is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    // DeviceGuard omitted
    return at::native::sparse_dim_sparse(self);
}
int64_t SparseCUDAType::_dimI(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_dimI is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    // DeviceGuard omitted
    return at::native::sparse_dim_sparse(self);
}
int64_t SparseCUDAType::dense_dim(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "dense_dim is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    // DeviceGuard omitted
    return at::native::dense_dim_sparse(self);
}
int64_t SparseCUDAType::_dimV(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_dimV is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    // DeviceGuard omitted
    return at::native::dense_dim_sparse(self);
}
int64_t SparseCUDAType::_nnz(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_nnz is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    // DeviceGuard omitted
    return at::native::_nnz_sparse(self);
}
Tensor SparseCUDAType::coalesce(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "coalesce is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::coalesce_sparse_cuda(self);
}
bool SparseCUDAType::is_coalesced(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    // DeviceGuard omitted
    return at::native::is_coalesced_sparse(self);
}
Tensor SparseCUDAType::_indices(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_indices is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    // DeviceGuard omitted
    return at::native::_indices_sparse(self);
}
Tensor SparseCUDAType::_values(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_values is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    // DeviceGuard omitted
    return at::native::_values_sparse(self);
}
Tensor & SparseCUDAType::_coalesced_(Tensor & self, bool coalesced) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "_coalesced_ is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    // DeviceGuard omitted
    return at::native::_coalesced_sparse_(self, coalesced);
}
Tensor SparseCUDAType::indices(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "indices is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    // DeviceGuard omitted
    return at::native::indices_sparse(self);
}
Tensor SparseCUDAType::values(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR(
            "values is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    // DeviceGuard omitted
    return at::native::values_sparse(self);
}
Tensor & SparseCUDAType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) {
#ifdef BUILD_NAMEDTENSOR
    if (out.has_names() || mat1.has_names() || mat2.has_names()) {
        AT_ERROR(
            "hspmm_out is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::hspmm_out_sparse_cuda(out, mat1, mat2);
}
Tensor SparseCUDAType::hspmm(const Tensor & mat1, const Tensor & mat2) {
#ifdef BUILD_NAMEDTENSOR
    if (mat1.has_names() || mat2.has_names()) {
        AT_ERROR(
            "hspmm is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(mat1));
    return at::native::hspmm_sparse_cuda(mat1, mat2);
}
Tensor & SparseCUDAType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || src.has_names()) {
        AT_ERROR(
            "copy_sparse_to_sparse_ is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::copy_sparse_(self, src, non_blocking);
}
Tensor SparseCUDAType::index_select(const Tensor & self, int64_t dim, const Tensor & index) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names() || index.has_names()) {
        AT_ERROR(
            "index_select is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_select_sparse(self, dim, index);
}

#ifndef USE_STATIC_DISPATCH
static auto registerer = torch::RegisterOperators()
  .op(torch::RegisterOperators::options()
    .schema("aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor")
    .kernel<Tensor (const Tensor &, const Tensor &, Scalar), &SparseCUDAType::add>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &, Scalar), &SparseCUDAType::add_>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &, const Tensor &, Scalar), &SparseCUDAType::add_out>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::div.Tensor(Tensor self, Tensor other) -> Tensor")
    .kernel<Tensor (const Tensor &, const Tensor &), &SparseCUDAType::div>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &), &SparseCUDAType::div_>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &, const Tensor &), &SparseCUDAType::div_out>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::empty.memory_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")
    .impl_unboxedOnlyKernel<Tensor (IntArrayRef, const TensorOptions &, c10::optional<MemoryFormat>), &SparseCUDAType::empty>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::log1p_(Tensor(a!) self) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &), &SparseCUDAType::log1p_>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &), &SparseCUDAType::log1p_out>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::mm(Tensor self, Tensor mat2) -> Tensor")
    .kernel<Tensor (const Tensor &, const Tensor &), &SparseCUDAType::mm>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &, const Tensor &), &SparseCUDAType::mm_out>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::mul.Tensor(Tensor self, Tensor other) -> Tensor")
    .kernel<Tensor (const Tensor &, const Tensor &), &SparseCUDAType::mul>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &), &SparseCUDAType::mul_>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &, const Tensor &), &SparseCUDAType::mul_out>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::narrow_copy(Tensor self, int dim, int start, int length) -> Tensor")
    .kernel<Tensor (const Tensor &, int64_t, int64_t, int64_t), &SparseCUDAType::narrow_copy>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::sspaddmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &, const Tensor &, const Tensor &, Scalar, Scalar), &SparseCUDAType::sspaddmm_out>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::native_norm(Tensor self, Scalar p=2) -> Tensor")
    .kernel<Tensor (const Tensor &, Scalar), &SparseCUDAType::native_norm>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::_sparse_sum_backward(Tensor grad, Tensor self, int[] dim) -> Tensor")
    .impl_unboxedOnlyKernel<Tensor (const Tensor &, const Tensor &, IntArrayRef), &SparseCUDAType::_sparse_sum_backward>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::clone(Tensor self) -> Tensor")
    .kernel<Tensor (const Tensor &), &SparseCUDAType::clone>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::resize_as_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &), &SparseCUDAType::resize_as_>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &, Scalar), &SparseCUDAType::pow_out>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor")
    .kernel<Tensor (const Tensor &, Scalar), &SparseCUDAType::pow>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::zero_(Tensor(a!) self) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &), &SparseCUDAType::zero_>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &, const Tensor &, Scalar), &SparseCUDAType::sub_out>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor")
    .kernel<Tensor (const Tensor &, const Tensor &, Scalar), &SparseCUDAType::sub>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &, Scalar), &SparseCUDAType::sub_>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &, const Tensor &, const Tensor &, Scalar, Scalar), &SparseCUDAType::addmm_out>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor")
    .kernel<Tensor (const Tensor &, const Tensor &, const Tensor &, Scalar, Scalar), &SparseCUDAType::addmm>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &, const Tensor &, Scalar, Scalar), &SparseCUDAType::addmm_>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor")
    .impl_unboxedOnlyKernel<Tensor (int64_t, int64_t, IntArrayRef, const TensorOptions &), &SparseCUDAType::_sparse_coo_tensor_with_dims>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, int[] size, Tensor indices, Tensor values, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor")
    .impl_unboxedOnlyKernel<Tensor (int64_t, int64_t, IntArrayRef, const Tensor &, const Tensor &, const TensorOptions &), &SparseCUDAType::_sparse_coo_tensor_with_dims_and_tensors>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::sparse_resize_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, IntArrayRef, int64_t, int64_t), &SparseCUDAType::sparse_resize_>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::sparse_resize_and_clear_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, IntArrayRef, int64_t, int64_t), &SparseCUDAType::sparse_resize_and_clear_>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::sparse_mask(Tensor self, Tensor mask) -> Tensor")
    .kernel<Tensor (const Tensor &, const Tensor &), &SparseCUDAType::sparse_mask>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::to_dense(Tensor self) -> Tensor")
    .kernel<Tensor (const Tensor &), &SparseCUDAType::to_dense>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::sparse_dim(Tensor self) -> int")
    .kernel<int64_t (const Tensor &), &SparseCUDAType::sparse_dim>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::_dimI(Tensor self) -> int")
    .kernel<int64_t (const Tensor &), &SparseCUDAType::_dimI>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::dense_dim(Tensor self) -> int")
    .kernel<int64_t (const Tensor &), &SparseCUDAType::dense_dim>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::_dimV(Tensor self) -> int")
    .kernel<int64_t (const Tensor &), &SparseCUDAType::_dimV>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::_nnz(Tensor self) -> int")
    .kernel<int64_t (const Tensor &), &SparseCUDAType::_nnz>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::coalesce(Tensor self) -> Tensor")
    .kernel<Tensor (const Tensor &), &SparseCUDAType::coalesce>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::is_coalesced(Tensor self) -> bool")
    .kernel<bool (const Tensor &), &SparseCUDAType::is_coalesced>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::_indices(Tensor(a) self) -> Tensor(a)")
    .impl_unboxedOnlyKernel<Tensor (const Tensor &), &SparseCUDAType::_indices>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::_values(Tensor(a) self) -> Tensor(a)")
    .impl_unboxedOnlyKernel<Tensor (const Tensor &), &SparseCUDAType::_values>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::_coalesced_(Tensor(a!) self, bool coalesced) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, bool), &SparseCUDAType::_coalesced_>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::indices(Tensor(a) self) -> Tensor(a)")
    .impl_unboxedOnlyKernel<Tensor (const Tensor &), &SparseCUDAType::indices>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::values(Tensor(a) self) -> Tensor(a)")
    .impl_unboxedOnlyKernel<Tensor (const Tensor &), &SparseCUDAType::values>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::hspmm.out(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &, const Tensor &), &SparseCUDAType::hspmm_out>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::hspmm(Tensor mat1, Tensor mat2) -> Tensor")
    .kernel<Tensor (const Tensor &, const Tensor &), &SparseCUDAType::hspmm>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::copy_sparse_to_sparse_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)")
    .impl_unboxedOnlyKernel<Tensor & (Tensor &, const Tensor &, bool), &SparseCUDAType::copy_sparse_to_sparse_>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
  .op(torch::RegisterOperators::options()
    .schema("aten::index_select(Tensor self, int dim, Tensor index) -> Tensor")
    .kernel<Tensor (const Tensor &, int64_t, const Tensor &), &SparseCUDAType::index_select>(TensorTypeId::SparseCUDATensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA));
#endif
}
